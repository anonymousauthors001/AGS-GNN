{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1abf8733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the current code to python script\n",
    "#jupyter nbconvert --to script KNNWeights.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bffbb0c",
   "metadata": {},
   "source": [
    "# Test Dataset to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c466d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "NUM_PROCESSORS=multiprocessing.cpu_count()\n",
    "# print(\"Cpu count: \",NUM_PROCESSORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b785488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#as it turned out interactive shell (like Jupyter cannot handle CPU multiprocessing well so check which medium the code is runing)\n",
    "#we will write code in Jupyter for understanding purposes but final execuation will be in shell\n",
    "from ipynb.fs.full.Utils import isnotebook\n",
    "from ipynb.fs.full.Dataset import generate_synthetic\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "import torch_geometric.utils.homophily as homophily\n",
    "import copy\n",
    "import ipynb.fs.full.utils.MoonGraph as MoonGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e61e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_sparse import SparseTensor\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "random.seed(12345)\n",
    "import numpy as np\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b0d46b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from multiprocessing.pool import ThreadPool, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61f71567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import get_data\n",
    "from ipynb.fs.full.Dataset import datasets as available_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89a622d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "#set default arguments here\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()    \n",
    "    parser.add_argument('--log_info', type=bool, default=True)\n",
    "    parser.add_argument('--pbar', type=bool, default=False)\n",
    "    parser.add_argument('--num_worker', type=int, default=0)\n",
    "    parser.add_argument('--dataset', type=str, default=\"karate\", choices=available_datasets)\n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3684fa",
   "metadata": {},
   "source": [
    "## KNN Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae7d866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNWeight():\n",
    "    \n",
    "    def __init__(self, data, metric='cosine', log = False, lambda1=0.25, lambda2=0.25, w1=1.0, w2=0.5, w3=0.1):\n",
    "        \n",
    "        self.N = N = data.num_nodes\n",
    "        self.E = E = data.num_edges\n",
    "        self.data = data\n",
    "        self.log = log\n",
    "        self.lambda1=lambda1\n",
    "        self.lambda2=lambda2\n",
    "        self.w1=w1\n",
    "        self.w2=w2\n",
    "        self.w3=w3\n",
    "        \n",
    "        \n",
    "        self.metric = metric\n",
    "        self.sign = 1\n",
    "        \n",
    "        if self.log:\n",
    "            print(\"Metric: \", metric)\n",
    "        \n",
    "        if metric=='cosine':\n",
    "            self.sign = -1\n",
    "            self.sim_func = cosine_similarity\n",
    "        elif metric=='euclidean':\n",
    "            self.sign = 1\n",
    "            self.sim_func = euclidean_distances\n",
    "        else:\n",
    "            print('error')\n",
    "            raise 'Not defined error'\n",
    "\n",
    "        self.adj = SparseTensor(\n",
    "            row=data.edge_index[0], col=data.edge_index[1],\n",
    "            value=torch.arange(E, device=data.edge_index.device),\n",
    "            sparse_sizes=(N, N))\n",
    "        \n",
    "    def node_weight(self,u):\n",
    "    \n",
    "        row, col, edge_index = self.adj[u,:].coo()   \n",
    "        \n",
    "        if len(col)==0:\n",
    "            return [],[]\n",
    "        \n",
    "        lambda1 = self.lambda1 #top 25% with probability 1\n",
    "        lambda2 = self.lambda2 #second 25% with probability 0.5 \n",
    "        \n",
    "        l1=math.ceil(len(col)*lambda1)\n",
    "        l2=min(len(col)-l1,math.ceil(len(col)*lambda2))        \n",
    "        l3=max(0,int(len(col)-l1-l2))\n",
    "        #print(len(col),l1, l2, l3)\n",
    "                        \n",
    "        target_class_sim = self.sim_func(self.data.x[u].view(1,-1), self.data.x[col.tolist()])        \n",
    "        #print(target_class_sim)\n",
    "        ind = np.argsort(self.sign*target_class_sim[0]) #-1*desending, normal will be ascending    \n",
    "        \n",
    "#         if len(col)>=10:\n",
    "#             ind=np.argpartition(self.sign*target_class_sim[0], kth=[l1,l2], axis=-1, kind='introselect', order=None)\n",
    "#         else:\n",
    "#             ind = np.argsort(self.sign*target_class_sim[0]) #-1*desending, normal will be ascending    \n",
    "                        \n",
    "#         print(u, row, col, edge_index)\n",
    "#         print(target_class_sim)\n",
    "#         print(ind)\n",
    "#         S_G = np.ones(l1, dtype=float)*1.0\n",
    "#         S_G = np.append(S_G, np.ones(l2, dtype=float)*0.5)\n",
    "#         if(l3>0):\n",
    "#             S_G = np.append(S_G, np.ones(l3, dtype=float)*0.1)\n",
    "\n",
    "        S_G = np.ones(l1, dtype=float)*self.w1\n",
    "        S_G = np.append(S_G, np.ones(l2, dtype=float)*self.w2)\n",
    "        \n",
    "        if(l3>0):\n",
    "            S_G = np.append(S_G, np.ones(l3, dtype=float)*self.w3)\n",
    "        \n",
    "        S_G = S_G.tolist()\n",
    "        \n",
    "#         S_G = list(range(1,len(col)+1))\n",
    "        S_edge = edge_index[ind].tolist()\n",
    "    \n",
    "        #print(S_G, S_edge)\n",
    "        \n",
    "        return S_G, S_edge\n",
    "\n",
    "    def get_knn_weight(self):\n",
    "        \n",
    "        if self.log:\n",
    "            pbar = tqdm(total=self.N)\n",
    "            pbar.set_description(f'Nodes')\n",
    "\n",
    "        edge_weight=[]\n",
    "        edge_index=[]\n",
    "\n",
    "        for u in range(self.N):            \n",
    "            weight, e_index = self.node_weight(u)\n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            \n",
    "            if self.log:\n",
    "                pbar.update(1)\n",
    "        \n",
    "        if self.log:\n",
    "            pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E\n",
    "        \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)\n",
    "        \n",
    "        return weight\n",
    "    \n",
    "    def process_block(self, list_u):\n",
    "        \n",
    "        #print(\"Processing :\",len(list_u), list_u[0], list_u[-1])\n",
    "        \n",
    "        edge_weight = []\n",
    "        edge_index = []\n",
    "        \n",
    "        for u in list_u:        \n",
    "            weight, e_index = self.node_weight(u)            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            \n",
    "        #print(\"Done :\",len(list_u), list_u[0], list_u[-1])\n",
    "            \n",
    "        return edge_weight, edge_index, len(list_u)\n",
    "    \n",
    "    #multiprocessing\n",
    "    def get_knn_weight_multiproces(self):\n",
    "        \n",
    "        edge_weight=[]\n",
    "        edge_index=[]        \n",
    "        \n",
    "        N = self.N\n",
    "        #N = 1000\n",
    "        \n",
    "        #elem_size=1000\n",
    "        #num_blocks = int(N/elem_size)\n",
    "        num_blocks = NUM_PROCESSORS\n",
    "        elem_size = int(N/num_blocks)\n",
    "        \n",
    "        \n",
    "        nodes = np.arange(num_blocks*elem_size).reshape(num_blocks,-1).tolist()\n",
    "        \n",
    "        if num_blocks*elem_size<N:\n",
    "            nodes.append(list(range(num_blocks*elem_size,N)))        \n",
    "        \n",
    "        pool_size = NUM_PROCESSORS        \n",
    "        if self.log:\n",
    "            print(\"Pool Size: \", pool_size)        \n",
    "        pool = Pool(pool_size)\n",
    "        \n",
    "        if self.log:\n",
    "            pbar = tqdm(total=N)\n",
    "            pbar.set_description(f'Nodes')  \n",
    "                \n",
    "        for (weight, e_index, num_el) in pool.imap_unordered(self.process_block, nodes):            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            \n",
    "            if self.log:\n",
    "                pbar.update(num_el)\n",
    "        \n",
    "        if self.log:\n",
    "            pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E        \n",
    "        \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)\n",
    "        \n",
    "        return weight\n",
    "    \n",
    "    \n",
    "    def compute_weights(self):   \n",
    "        #if isnotebook():\n",
    "        #weight = self.get_knn_weight()\n",
    "        \n",
    "        if self.data.num_nodes<10000:\n",
    "            weight = self.get_knn_weight()    \n",
    "        else:\n",
    "            weight = self.get_knn_weight_multiproces()\n",
    "        \n",
    "        return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a49240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, dataset = get_data('Cora', log=False, h_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9502907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn_weight = KNNWeight(data, 'cosine', log = True)\n",
    "# knn_weight.node_weight(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ec3e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun():\n",
    "    \n",
    "    for d in range(10,101, 10):    \n",
    "        #d = 25\n",
    "        h=0.25\n",
    "\n",
    "        data, dataset = get_data('Cora', log=False)\n",
    "        data = generate_synthetic(data, d=d, h=h, train=0.1, random_state=None, log=False)\n",
    "\n",
    "        Nh = homophily(data.edge_index, data.y, method='node')\n",
    "        Eh = homophily(data.edge_index, data.y, method='edge')\n",
    "        Einh = homophily(data.edge_index, data.y, method='edge_insensitive')\n",
    "        E = len(data.edge_index[0])\n",
    "\n",
    "        knn_weight = KNNWeight(data, 'cosine')\n",
    "        data.weight = knn_weight.compute_weights()\n",
    "\n",
    "        if 'weight' in data:\n",
    "            cp_data= copy.deepcopy(data)\n",
    "            G = to_networkx(cp_data, to_undirected=True, edge_attrs=['weight'])\n",
    "            to_remove = [(a,b) for a, b, attrs in G.edges(data=True) if attrs[\"weight\"] <1.0 ]\n",
    "            G.remove_edges_from(to_remove)\n",
    "            updated_data = from_networkx(G)\n",
    "\n",
    "            up_E = len(updated_data.edge_index[0])\n",
    "\n",
    "            up_Nh = homophily(updated_data.edge_index, cp_data.y, method='node')\n",
    "            up_Eh = homophily(updated_data.edge_index, cp_data.y, method='edge')\n",
    "            up_Einh = homophily(updated_data.edge_index, cp_data.y, method='edge_insensitive')\n",
    "\n",
    "\n",
    "        print('{} {} {} {} {} {} {} {} {} {}'.format(d, h, Nh, Eh, Einh, E, up_Nh, up_Eh, up_Einh, up_E))\n",
    "\n",
    "    return\n",
    "#fun()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddbb922",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec49c506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:  /scratch/gilbreth/das90/Dataset/\n",
      "Result directory: /scratch/gilbreth/das90/Dataset/RESULTS/\n",
      "\n",
      "Dataset: KarateClub():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 34\n",
      "Number of classes: 4\n",
      "\n",
      "Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34], val_mask=[34], test_mask=[34])\n",
      "===========================================================================================================\n",
      "Number of nodes: 34\n",
      "Number of edges: 156\n",
      "Average node degree: 4.59\n",
      "Number of training nodes: 4\n",
      "Training node label rate: 0.12\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n",
      "Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34], val_mask=[34], test_mask=[34])\n",
      "Metric:  cosine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nodes: 100%|██████████| 34/34 [00:00<00:00, 2425.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time:  0.01596689224243164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "    \n",
    "    args.dataset = 'karate'\n",
    "    \n",
    "    data, dataset = get_data(args.dataset)\n",
    "#     data = generate_synthetic(data, d=100, h=0.25, train=0.1, random_state=None, log=False)\n",
    "    \n",
    "    print(data)\n",
    "    \n",
    "#     print(\"Node Homophily:\", homophily(data.edge_index, data.y, method='node'))\n",
    "#     print(\"Edge Homophily:\", homophily(data.edge_index, data.y, method='edge'))\n",
    "#     print(\"Edge_insensitive Homophily:\", homophily(data.edge_index, data.y, method='edge_insensitive'))    \n",
    "    \n",
    "    knn_weight = KNNWeight(data, 'cosine', log = True)\n",
    "    #S_G, S_edge = knn_weight.lazy_greedy_weight(0); print(S_G); print(S_edge);\n",
    "    #S_G, S_edge = knn_weight.process_block([0,1,2]); print(S_G); print(S_edge);\n",
    "\n",
    "    start = time.time()    \n",
    "    data.weight = knn_weight.compute_weights()\n",
    "    end = time.time()\n",
    "    print(\"Execution time: \", end-start)\n",
    "    \n",
    "#     if 'weight' in data:\n",
    "#         cp_data= copy.deepcopy(data)\n",
    "#         G = to_networkx(cp_data, to_undirected=True, edge_attrs=['weight'])\n",
    "#         to_remove = [(a,b) for a, b, attrs in G.edges(data=True) if attrs[\"weight\"] <1.0 ]\n",
    "#         G.remove_edges_from(to_remove)\n",
    "#         updated_data = from_networkx(G)\n",
    "        \n",
    "#         updated_data = from_networkx(G, group_edge_attrs=['weight'])\n",
    "#         updated_data.weight = updated_data.edge_attr.view(-1)\n",
    "        \n",
    "#         row, col = updated_data.edge_index\n",
    "#         updated_data.edge_index = torch.stack((torch.cat((row, col),dim=0), torch.cat((col, row),dim=0)),dim=0)\n",
    "#         updated_data.weight = torch.cat((updated_data.weight, updated_data.weight),dim=0)\n",
    "        \n",
    "        \n",
    "#         print(updated_data)\n",
    "\n",
    "#         print(\"Node Homophily:\", homophily(updated_data.edge_index, cp_data.y, method='node'))\n",
    "#         print(\"Edge Homophily:\", homophily(updated_data.edge_index, cp_data.y, method='edge'))\n",
    "#         print(\"Edge_insensitive Homophily:\", homophily(updated_data.edge_index, cp_data.y, method='edge_insensitive'))    \n",
    "        \n",
    "    None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
