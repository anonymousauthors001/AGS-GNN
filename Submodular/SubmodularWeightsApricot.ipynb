{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bffbb0c",
   "metadata": {},
   "source": [
    "# Test Dataset to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c466d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "NUM_PROCESSORS=multiprocessing.cpu_count()\n",
    "#print(\"Cpu count: \",NUM_PROCESSORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5141598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#as it turned out interactive shell (like Jupyter cannot handle CPU multiprocessing well so check which medium the code is runing)\n",
    "#we will write code in Jupyter for understanding purposes but final execuation will be in shell\n",
    "from ipynb.fs.full.Utils import isnotebook\n",
    "from ipynb.fs.full.Dataset import get_data, generate_synthetic\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "import torch_geometric.utils.homophily as homophily\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f8e61e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_sparse import SparseTensor\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "\n",
    "import random\n",
    "random.seed(12345)\n",
    "import numpy as np\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1b0d46b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing.pool import ThreadPool, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6b209a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from apricot import FeatureBasedSelection, MaxCoverageSelection, FacilityLocationSelection\n",
    "from apricot import GraphCutSelection, SumRedundancySelection, SaturatedCoverageSelection, MixtureSelection\n",
    "from apricot import BaseSelection\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "768b3f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomGraphBased import CustomGraphBased\n",
    "from CustomFeatureBased import FeatureBasedSelection\n",
    "from CustomMaxCoverage import MaxCoverageSelection\n",
    "from CustomFacilityLocation import FacilityLocationSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b46e4897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = numpy.exp(numpy.random.randn(1000, 100))\n",
    "# model = FacilityLocationSelection(999, optimizer='approximate-lazy')\n",
    "# start = time.time()\n",
    "# model.fit(X)\n",
    "# print(\"Time:\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e7f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00f12dcc",
   "metadata": {},
   "source": [
    "# Apricot Facility function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8bb0c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubModularWeightApricot():\n",
    "    \n",
    "    def __init__(self, data, sub_func='facility', log=True, metric='cosine', concave_func='sqrt', lambda1=0.25, lambda2=0.25, w1=1.0, w2=0.5, w3=0.1):\n",
    "        \n",
    "        self.N = N = data.num_nodes\n",
    "        self.E = E = data.num_edges\n",
    "        self.data = data\n",
    "        self.log = log\n",
    "        \n",
    "        self.lambda1=lambda1\n",
    "        self.lambda2=lambda2\n",
    "        self.w1=w1\n",
    "        self.w2=w2\n",
    "        self.w3=w3\n",
    "        \n",
    "        self.metric = metric\n",
    "        self.sub_func = sub_func\n",
    "        self.concave_func=concave_func\n",
    "        \n",
    "        if self.log:\n",
    "            print(self.metric)\n",
    "\n",
    "        self.adj = SparseTensor(\n",
    "            row=data.edge_index[0], col=data.edge_index[1],\n",
    "            value=torch.arange(E, device=data.edge_index.device),\n",
    "            sparse_sizes=(N, N))\n",
    "        \n",
    "    def lazy_greedy_weight(self,u):\n",
    "    \n",
    "        row, col, edge_index = self.adj[u,:].coo()\n",
    "        \n",
    "        if len(col)==0:\n",
    "            return [],[]\n",
    "                \n",
    "#         print(len(row),row)\n",
    "#         print(len(col),col)\n",
    "#         print(len(edge_index),edge_index)\n",
    "            \n",
    "        vertices = [u]+col.tolist()\n",
    "        \n",
    "        lambda1 = self.lambda1 #top 25% with probability 1\n",
    "        lambda2 = self.lambda2 #second 25% with probability 0.5         \n",
    "        l1=math.ceil(len(col)*lambda1)\n",
    "        l2=min(len(col)-l1,math.ceil(len(col)*lambda2))\n",
    "        l3=max(0,int(len(col)-l1-l2))\n",
    "                \n",
    "#         print(l1,l2)\n",
    "#         n_jobs=NUM_PROCESSORS\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        if self.sub_func =='facility':\n",
    "            model = FacilityLocationSelection(l1+l2, initial_subset=[0], metric=self.metric, optimizer='approximate-lazy')\n",
    "        elif self.sub_func =='graph':        \n",
    "            model = CustomGraphBased(l1+l2, initial_subset=[0], metric=self.metric, optimizer='approximate-lazy')\n",
    "        elif self.sub_func =='feature':        \n",
    "            model = FeatureBasedSelection(l1+l2, initial_subset=[0], concave_func=self.concave_func, optimizer='approximate-lazy')\n",
    "        elif self.sub_func =='coverage':\n",
    "            model = MaxCoverageSelection(l1+l2, initial_subset=[0], optimizer='approximate-lazy')\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "    \n",
    "        end = time.time()\n",
    "        \n",
    "        \n",
    "        start2 = time.time()\n",
    "        #model.fit(self.data.x[vertices])\n",
    "        model.fit(self.data.x[vertices].numpy())\n",
    "        end2 = time.time()\n",
    "\n",
    "#         print(\"init:\",end-start)\n",
    "#         print(\"Fit:\",end2-start2)\n",
    "                \n",
    "#         \n",
    "        ranks = model.ranking\n",
    "        \n",
    "#         print(vertices)\n",
    "#         print(ranks, len(ranks))\n",
    "        \n",
    "        set1 = set(range(1,len(vertices)))\n",
    "        set2 = set(ranks)\n",
    "        other_ranks = list(set1 - set2)\n",
    "        \n",
    "#         print(other_ranks)        \n",
    "        \n",
    "        v2i={i:j for i,j in zip(vertices, range(len(vertices)))}\n",
    "        i2v={value:key for key, value in v2i.items()}\n",
    "        v2e={key.item():value.item() for key, value in zip(col,edge_index)}\n",
    "        \n",
    "        S_G=[]\n",
    "        S_edge=[]\n",
    "        \n",
    "        rank=1\n",
    "        \n",
    "        for r in ranks:\n",
    "            if rank <= l1:\n",
    "                S_G.append(self.w1)                \n",
    "            elif rank<=l1+l2:\n",
    "                S_G.append(self.w2)\n",
    "            else:\n",
    "                S_G.append(self.w3)\n",
    "            S_edge.append(v2e[i2v[r]])\n",
    "            rank+=1\n",
    "        \n",
    "        for r in other_ranks:\n",
    "            S_G.append(self.w3)\n",
    "            S_edge.append(v2e[i2v[r]])\n",
    "            rank+=1                    \n",
    "            \n",
    "        return S_G, S_edge\n",
    "\n",
    "    def get_submodular_weight(self):\n",
    "        \n",
    "        if self.log:\n",
    "            pbar = tqdm(total=self.N)\n",
    "            pbar.set_description(f'Nodes')\n",
    "\n",
    "        edge_weight=[]\n",
    "        edge_index=[]\n",
    "\n",
    "        for u in range(self.N):\n",
    "            \n",
    "            weight, e_index = self.lazy_greedy_weight(u)\n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            if self.log:\n",
    "                pbar.update(1)\n",
    "        if self.log:\n",
    "            pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E\n",
    "        \n",
    "        weight=torch.zeros(len(edge_weight))\n",
    "        for e,w in zip(edge_index,edge_weight):\n",
    "            weight[e]=w\n",
    "\n",
    "        return weight\n",
    "    \n",
    "    def process_block(self, list_u):\n",
    "        \n",
    "        #print(\"Processing :\",len(list_u), list_u[0], list_u[-1])\n",
    "        \n",
    "        edge_weight = []\n",
    "        edge_index = []\n",
    "        \n",
    "        for u in list_u:        \n",
    "            weight, e_index = self.lazy_greedy_weight(u)            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            \n",
    "        #print(\"Done :\",len(list_u), list_u[0], list_u[-1])\n",
    "            \n",
    "        return edge_weight, edge_index, len(list_u) \n",
    "    \n",
    "    #multiprocessing\n",
    "    def get_submodular_weight_multiproces(self):\n",
    "        \n",
    "        edge_weight=[]\n",
    "        edge_index=[]        \n",
    "        \n",
    "        N = self.N\n",
    "        num_blocks = NUM_PROCESSORS\n",
    "        elem_size = int(N/num_blocks)\n",
    "                \n",
    "        nodes = np.arange(num_blocks*elem_size).reshape(num_blocks,-1).tolist()\n",
    "        if num_blocks*elem_size<N:\n",
    "            nodes.append(list(range(num_blocks*elem_size,N)))        \n",
    "        \n",
    "        pool_size = NUM_PROCESSORS        \n",
    "        \n",
    "        if self.log:\n",
    "            print(\"Pool Size: \", pool_size)        \n",
    "        pool = Pool(pool_size)\n",
    "        \n",
    "        if self.log:\n",
    "            pbar = tqdm(total=N)\n",
    "            pbar.set_description(f'Nodes')  \n",
    "                \n",
    "        for (weight, e_index, num_el) in pool.imap_unordered(self.process_block, nodes):            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            if self.log:\n",
    "                pbar.update(num_el)\n",
    "        if self.log:\n",
    "            pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E\n",
    "                \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)        \n",
    "        \n",
    "        return weight\n",
    "    \n",
    "    \n",
    "    def compute_weights(self):\n",
    "        \n",
    "        weight = self.get_submodular_weight_multiproces()\n",
    "        \n",
    "#         if data.num_nodes<10000:\n",
    "#             weight = self.get_submodular_weight()    \n",
    "#         else:\n",
    "#             weight = self.get_submodular_weight_multiproces()\n",
    "        \n",
    "        return weight\n",
    "    \n",
    "# data, dataset = get_data('karate', log=False,h_score=False)\n",
    "# submodular_weight = SubModularWeightApricot(data, log=True, sub_func='facility', metric='euclidean')\n",
    "\n",
    "# start = time.time()\n",
    "# #submodular_weight.get_submodular_weight()\n",
    "# # submodular_weight.lazy_greedy_weight(0)\n",
    "# submodular_weight.compute_weights()\n",
    "\n",
    "# # for i in range(10):\n",
    "# #      submodular_weight.lazy_greedy_weight(i)\n",
    "# end = time.time()\n",
    "# print(\"Execution time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a410f9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c2da5f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, dataset = get_data('Cora', log=False,h_score=True)\n",
    "# submodular_weight = SubModularWeightApricot(data)\n",
    "# start = time.time()\n",
    "# # submodular_weight.process_block(list(range(30)))\n",
    "# # submodular_weight.compute_weights()\n",
    "# submodular_weight.get_submodular_weight()    \n",
    "# end = time.time()\n",
    "# print(\"Execution time: \", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddbb922",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ec49c506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euclidean\n",
      "Pool Size:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nodes: 100%|██████████| 34/34 [00:01<00:00, 31.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time:  1.4591522216796875\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "    \n",
    "    data, dataset = get_data('karate', log=False,h_score=False)\n",
    "    submodular_weight = SubModularWeightApricot(data, log=True, sub_func='facility', metric='euclidean')\n",
    "\n",
    "    start = time.time()\n",
    "    #submodular_weight.get_submodular_weight()\n",
    "    # submodular_weight.lazy_greedy_weight(0)\n",
    "    submodular_weight.compute_weights()\n",
    "\n",
    "    # for i in range(10):\n",
    "    #      submodular_weight.lazy_greedy_weight(i)\n",
    "    end = time.time()\n",
    "    print(\"Execution time: \", end-start)\n",
    "    \n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "52b030af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = numpy.exp(numpy.random.randn(4000, 602))\n",
    "\n",
    "# model = CustomSubmodularSelector(3999, initial_subset=[0], metric='cosine', optimizer='approximate-lazy')\n",
    "# start = time.time()\n",
    "# model.fit(X)\n",
    "# print(\"Time:\", time.time()-start)\n",
    "\n",
    "# model.ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a289772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    "# from apricot import FacilityLocationSelection\n",
    "\n",
    "# X = numpy.random.normal(20, 1, size=(10000, 1000))\n",
    "# # X_reordered = FacilityLocationSelection(100, initial_subset=[0]).fit_transform(X)\n",
    "\n",
    "# start = time.time()\n",
    "# model = FacilityLocationSelection(1000).fit(X)\n",
    "# X_reordered2 = X[model.ranking]\n",
    "# end = time.time()\n",
    "\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ab0d2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import numpy as np\n",
    "# from apricot import FacilityLocationSelection\n",
    "# from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "# #X = numpy.random.normal(2, 1, size=(10000, 1000))\n",
    "\n",
    "# X =np.array([[0,0],[0.8,0.0],[0.2,0.6]])\n",
    "\n",
    "# sim = euclidean_distances(X)\n",
    "\n",
    "# print(sim)\n",
    "# sim = np.max(sim)-sim\n",
    "# print(sim)\n",
    "\n",
    "# #print(np.sum(sim,axis=0))\n",
    "\n",
    "\n",
    "# # X_reordered = FacilityLocationSelection(100, initial_subset=[0]).fit_transform(X)\n",
    "\n",
    "# start = time.time()\n",
    "# model = FacilityLocationSelection(3, metric='euclidean',optimizer=\"naive\").fit(X)\n",
    "\n",
    "# print(model.ranking)\n",
    "# print(model.gains)\n",
    "\n",
    "# #X_reordered2 = X[model.ranking]\n",
    "# end = time.time()\n",
    "\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f61a208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def calculate_pairwise_distances(X, Y=None, metric='precomputed', n_neighbors=None):\n",
    "    if metric in ('precomputed', 'ignore'):\n",
    "        return X\n",
    "\n",
    "    if n_neighbors is None:\n",
    "        if metric == 'euclidean':\n",
    "            X_pairwise = pairwise_distances(X, Y=Y, metric=metric, squared=True)\n",
    "        elif metric == 'correlation' or metric == 'cosine':\n",
    "            # An in-place version of:\n",
    "            # X_pairwise = 1 - (1 - pairwise_distances(X, metric=metric)) ** 2\n",
    "            \n",
    "            X_pairwise = pairwise_distances(X, Y=Y, metric=metric)\n",
    "            X_pairwise = numpy.subtract(1, X_pairwise, out=X_pairwise)\n",
    "            X_pairwise = numpy.square(X_pairwise, out=X_pairwise)\n",
    "            X_pairwise = numpy.subtract(1, X_pairwise, out=X_pairwise)\n",
    "        else:\n",
    "            X_pairwise = pairwise_distances(X, Y=Y, metric=metric)\n",
    "    else:\n",
    "        if metric == 'correlation' or metric == 'cosine':\n",
    "            # An in-place version of:\n",
    "            # X = 1 - (1 - pairwise_distances(X, metric=metric)) ** 2\n",
    "\n",
    "            X = pairwise_distances(X, Y=Y, metric=metric)\n",
    "            X = numpy.subtract(1, X, out=X)\n",
    "            X = numpy.square(X, out=X)\n",
    "            X = numpy.subtract(1, X, out=X)\n",
    "            metric = 'precomputed'\n",
    "\n",
    "        if isinstance(n_neighbors, int):\n",
    "            X_pairwise = KNeighborsTransformer(\n",
    "                n_neighbors=n_neighbors, metric=metric\n",
    "                ).fit_transform(X)\n",
    "\n",
    "        elif isinstance(n_neighbors, KNeighborsTransformer):\n",
    "            X_pairwise = n_neighbors.fit_transform(X)\n",
    "\n",
    "    if metric == 'correlation' or metric == 'cosine':\n",
    "        if isinstance(X_pairwise, csr_matrix):\n",
    "            X_pairwise.data = numpy.subtract(1, X_pairwise.data, \n",
    "                out=X_pairwise.data)\n",
    "        else:\n",
    "            X_pairwise = numpy.subtract(1, X_pairwise,\n",
    "                out=X_pairwise)\n",
    "    else:\n",
    "        if isinstance(X_pairwise, csr_matrix):\n",
    "            X_pairwise.data = numpy.subtract(X_pairwise.max(),\n",
    "                X_pairwise.data, out=X_pairwise.data)\n",
    "        else:\n",
    "            X_pairwise = numpy.subtract(X_pairwise.max(), X_pairwise,\n",
    "                out=X_pairwise)\n",
    "\n",
    "    return X_pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8065764f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 5 0 2 4 6]\n",
      "[488.  48.  27.   1.   1.   1.   1.]\n",
      "1.2230839729309082\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from apricot import FacilityLocationSelection\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "\n",
    "X =np.array([[0,0],[1,0],[2,0],[5,0],[7,0],[8,0],[9,0]])\n",
    "\n",
    "start = time.time()\n",
    "model = FacilityLocationSelection(7, metric='euclidean',optimizer=\"naive\").fit(X)\n",
    "# model = FacilityLocationSelection(4, initial_subset=[0], metric='euclidean',optimizer=\"naive\").fit(X)\n",
    "\n",
    "print(model.ranking)\n",
    "print(model.gains)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "56f5eda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 & 80 & 77 & 56 & 32 & 17 & 0\\\\\n",
      "80 & 81 & 80 & 65 & 45 & 32 & 17\\\\\n",
      "77 & 80 & 81 & 72 & 56 & 45 & 32\\\\\n",
      "56 & 65 & 72 & 81 & 77 & 72 & 65\\\\\n",
      "32 & 45 & 56 & 77 & 81 & 80 & 77\\\\\n",
      "17 & 32 & 45 & 72 & 80 & 81 & 80\\\\\n",
      "0 & 17 & 32 & 65 & 77 & 80 & 81\\\\\n"
     ]
    }
   ],
   "source": [
    "X_pair = calculate_pairwise_distances(X, metric='euclidean')\n",
    "\n",
    "X_pair\n",
    "\n",
    "\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(X)):\n",
    "        print(int(X_pair[i,j]),end=\"\")\n",
    "        if j<len(X)-1:\n",
    "            print(end= \" & \")\n",
    "    print(\"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0e14fb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[81., 80., 77., 56., 32., 17.,  0.],\n",
       "       [80., 81., 80., 65., 45., 32., 17.],\n",
       "       [77., 80., 81., 72., 56., 45., 32.],\n",
       "       [56., 65., 72., 81., 77., 72., 65.],\n",
       "       [32., 45., 56., 77., 81., 80., 77.],\n",
       "       [17., 32., 45., 72., 80., 81., 80.],\n",
       "       [ 0., 17., 32., 65., 77., 80., 81.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pairwise = pairwise_distances(X, metric='euclidean', squared=False)\n",
    "\n",
    "np.power(X_pairwise,2).max()-np.power(X_pairwise,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2a67fcce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.max(X_pair[[3,5]],axis=0))-sum(np.max(X_pair[[3]],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3df350e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 & 0\\\\\n",
      "1 & 0\\\\\n",
      "2 & 0\\\\\n",
      "5 & 0\\\\\n",
      "7 & 0\\\\\n",
      "8 & 0\\\\\n",
      "9 & 0\\\\\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X)):\n",
    "    for j in range(2):\n",
    "        print(int(X[i,j]),end=\"\")\n",
    "        if j<1:\n",
    "            print(end= \" & \")\n",
    "    print(\"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "70b908f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(np.max(sim[:2],axis=0))-sum(np.max(sim[:1],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cfbd098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim = np.array(\n",
    "#     [[0.72, 0.08, 0.32],\n",
    "#      [0.08, 0.72, 0.  ],\n",
    "#      [0.32, 0.,   0.72]])\n",
    "\n",
    "# sum(np.max(sim,axis=0))-sum(np.max(sim[:2],axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
