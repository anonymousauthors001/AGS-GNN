{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bffbb0c",
   "metadata": {},
   "source": [
    "# Test Dataset to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c466d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2b785488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#as it turned out interactive shell (like Jupyter cannot handle CPU multiprocessing well so check which medium the code is runing)\n",
    "#we will write code in Jupyter for understanding purposes but final execuation will be in shell\n",
    "from ipynb.fs.full.Utils import isnotebook\n",
    "from ipynb.fs.full.Dataset import get_data, generate_synthetic\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "import torch_geometric.utils.homophily as homophily\n",
    "import copy\n",
    "import ipynb.fs.full.utils.MoonGraph as MoonGraph\n",
    "import logging\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torch_geometric.utils import add_self_loops\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0ebdc9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "from ipynb.fs.full.Dataset import datasets as available_datasets\n",
    "\n",
    "#set default arguments here\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()    \n",
    "    parser.add_argument('--log_info', type=bool, default=True)\n",
    "    parser.add_argument('--pbar', type=bool, default=False)\n",
    "    parser.add_argument('--num_worker', type=int, default=0)\n",
    "    parser.add_argument('--dataset', type=str, default=\"karate\", choices=available_datasets)\n",
    "    parser.add_argument('--epochs', type=int, default=150)\n",
    "    parser.add_argument('--batch_size', type=int, default=4096)\n",
    "    parser.add_argument('--num_neurons', type=int, default=64)\n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f8e61e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_sparse import SparseTensor\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "random.seed(12345)\n",
    "import numpy as np\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1b0d46b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from multiprocessing.pool import ThreadPool, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3ebe5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GINConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a8cadff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, dataset = get_data('Cora', log=False)\n",
    "# data\n",
    "\n",
    "# adj_mat = torch.zeros((data.num_nodes,data.num_nodes))\n",
    "# edges = data.edge_index.t()\n",
    "# adj_mat[edges[:,0], edges[:,1]] = 1\n",
    "\n",
    "# adj_train = adj_mat[data.train_mask].t()[data.train_mask].t()\n",
    "# adj_validation = adj_mat[data.val_mask].t()[data.val_mask].t()\n",
    "# adj_test = adj_mat[data.test_mask].t()[data.test_mask].t()\n",
    "\n",
    "# print(adj_train.shape)\n",
    "# print(adj_validation.shape)\n",
    "# print(adj_test.shape)\n",
    "\n",
    "# G_train=Data(edge_index=(adj_train.nonzero()).t(), x=data.x[data.train_mask], y=data.y[data.train_mask])\n",
    "# G_val=Data(edge_index=(adj_validation.nonzero()).t(), x=data.x[data.val_mask], y=data.y[data.val_mask])\n",
    "# G_test=Data(edge_index=(adj_test.nonzero()).t(), x=data.x[data.test_mask], y=data.y[data.test_mask])\n",
    "# G_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9c114396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_adj(adj_mat, task, percent=2):\n",
    "    \"\"\" Returns the corrupted version of the adjacency matrix \"\"\"\n",
    "    if task == 'link':\n",
    "        edges = adj_mat.triu().nonzero()\n",
    "        num_edges = edges.shape[0]\n",
    "        num_to_corrupt = int(percent/100.0 * num_edges)\n",
    "        random_corruption = np.random.randint(num_edges, size=num_to_corrupt)\n",
    "        adj_mat_corrupted = adj_mat.clone()\n",
    "        false_edges, false_non_edges = [], []\n",
    "        #Edge Corruption\n",
    "        for ed in edges[random_corruption]:\n",
    "            adj_mat_corrupted[ed[0], ed[1]] = 0\n",
    "            adj_mat_corrupted[ed[1], ed[0]] = 0\n",
    "            false_non_edges.append(ed.tolist())\n",
    "        #Non Edge Corruption\n",
    "        random_non_edge_corruption = list(np.random.randint(adj_mat.shape[0], size = 6*num_to_corrupt))\n",
    "        non_edge_to_corrupt = []\n",
    "        for k in range(len(random_non_edge_corruption)-1):\n",
    "            to_check = [random_non_edge_corruption[k], random_non_edge_corruption[k+1]]\n",
    "            if to_check not in edges.tolist():\n",
    "                non_edge_to_corrupt.append(to_check)\n",
    "            if len(non_edge_to_corrupt) == num_to_corrupt:\n",
    "                break\n",
    "        non_edge_to_corrupt = torch.Tensor(non_edge_to_corrupt).type(torch.int16)\n",
    "        for n_ed in non_edge_to_corrupt:\n",
    "            adj_mat_corrupted[n_ed[0], n_ed[1]] = 1\n",
    "            adj_mat_corrupted[n_ed[1], n_ed[0]] = 1\n",
    "            false_edges.append(n_ed.tolist())\n",
    "    return adj_mat_corrupted, false_edges, false_non_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "31ef8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_equal_number_edges_non_edges(adj_mat, false_non_edges, false_edges, small_samples):\n",
    "    edges = adj_mat.nonzero()        \n",
    "    num_edges = edges.shape[0]\n",
    "    inverse_adj_mat = 1 - adj_mat\n",
    "    non_edges = inverse_adj_mat.nonzero()\n",
    "    num_non_edges  = non_edges.shape[0]\n",
    "    \n",
    "    edges_sampled = edges[np.random.randint(num_edges, size=min(num_edges,small_samples))]\n",
    "    non_edges_sampled = non_edges[np.random.randint(num_non_edges, size=min(num_non_edges,small_samples))]\n",
    "    final_edges = []\n",
    "    final_non_edges = []\n",
    "    for ed in edges_sampled.tolist():\n",
    "        if ed not in false_edges:\n",
    "            final_edges.append(ed)\n",
    "    final_edges += false_non_edges\n",
    "    for n_ed in non_edges_sampled.tolist():\n",
    "        if n_ed not in false_non_edges:\n",
    "            final_non_edges.append(n_ed)\n",
    "    final_non_edges += false_edges\n",
    "\n",
    "    return final_edges, final_non_edges\n",
    "\n",
    "#edges, non_edges = sample_equal_number_edges_non_edges(adj_train, false_non_edges=[], false_edges=[], small_samples=10, N=34)\n",
    "\n",
    "\n",
    "def train_sample_equal_number_edges_non_edges(adj_mat, false_non_edges, false_edges, small_samples, N = None):    \n",
    "    edges = adj_mat.nonzero()   \n",
    "    \n",
    "#     print(edges)    \n",
    "#     print(edges.shape[0])\n",
    "    \n",
    "    factor = 2\n",
    "    if edges.shape[0]< factor*N:\n",
    "        times = math.ceil(factor*N/edges.shape[0])\n",
    "#         print(times)\n",
    "        \n",
    "        edges = torch.tile(edges, (times, 1))        \n",
    "#         print(edges)\n",
    "    \n",
    "    edge_type = torch.zeros(edges.shape[0], dtype=torch.bool)\n",
    "    \n",
    "    if N is not None:\n",
    "        indentity_edges = torch.cat((torch.LongTensor(range(N)).view(N,1), torch.LongTensor(range(N)).view(N,1)), dim=1)\n",
    "        edges = torch.cat((edges, indentity_edges), dim=0)      \n",
    "        edge_type = torch.cat((edge_type, torch.ones(indentity_edges.shape[0], dtype=torch.bool)), dim=0)                \n",
    "    \n",
    "    num_edges = edges.shape[0]\n",
    "    inverse_adj_mat = 1 - adj_mat\n",
    "    non_edges = inverse_adj_mat.nonzero()\n",
    "    num_non_edges  = non_edges.shape[0]\n",
    "    \n",
    "    equal_sample_size = min(num_edges, small_samples, num_non_edges)\n",
    "    \n",
    "    edge_sample_index = np.random.randint(num_edges, size=equal_sample_size)\n",
    "    \n",
    "    edges_sampled = edges[edge_sample_index]\n",
    "    edges_type_sampled = edge_type[edge_sample_index]\n",
    "     \n",
    "    identity_edges = edges_sampled[edges_type_sampled]\n",
    "    edges_sampled = edges_sampled[~edges_type_sampled]\n",
    "    \n",
    "    non_edges_sampled = non_edges[np.random.randint(num_non_edges, size=equal_sample_size)]\n",
    "    edges_type_sampled = torch.cat((torch.zeros(edges_sampled.shape[0], dtype=torch.bool),                                    \n",
    "                                    torch.zeros(non_edges_sampled.shape[0], dtype=torch.bool),\n",
    "                                    torch.ones(identity_edges.shape[0], dtype=torch.bool)), dim=0)\n",
    "        \n",
    "    final_edges = []\n",
    "    final_non_edges = []\n",
    "    \n",
    "    ##need to update for edge_typye_sample\n",
    "    for ed in edges_sampled.tolist():\n",
    "        if ed not in false_edges:\n",
    "            final_edges.append(ed)\n",
    "    \n",
    "    final_edges += false_non_edges\n",
    "    for n_ed in non_edges_sampled.tolist():\n",
    "        if n_ed not in false_non_edges:\n",
    "            final_non_edges.append(n_ed)\n",
    "    final_non_edges += false_edges\n",
    "\n",
    "    return final_edges, final_non_edges, identity_edges, edges_type_sampled\n",
    "\n",
    "# edges, non_edges, identity_edges, edges_type = train_sample_equal_number_edges_non_edges(adj_train, false_non_edges=[], false_edges=[], small_samples=10, N=34)\n",
    "# edges, non_edges, identity_edges, edges_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "53fe5c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatcher(object):\n",
    "    def __init__(self, batch_size, n_examples, shuffle=True):\n",
    "        assert batch_size <= n_examples, \"Error: batch_size is larger than n_examples\"\n",
    "        self.batch_size = batch_size\n",
    "        self.n_examples = n_examples\n",
    "        self.shuffle = shuffle\n",
    "        logging.info(\"batch_size={}, n_examples={}\".format(batch_size, n_examples))\n",
    "\n",
    "        self.idxs = np.arange(self.n_examples)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.idxs)\n",
    "        self.current_start = 0\n",
    "\n",
    "    def get_one_batch(self):\n",
    "        self.idxs = np.arange(self.n_examples)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.idxs)\n",
    "        self.current_start = 0\n",
    "        while self.current_start < self.n_examples:\n",
    "            batch_idxs = self.idxs[self.current_start:self.current_start+self.batch_size]\n",
    "            self.current_start += self.batch_size\n",
    "            yield torch.LongTensor(batch_idxs)\n",
    "            \n",
    "# train_batcher = MiniBatcher(2, 10)\n",
    "# for i in range(3):\n",
    "#     for train_idxs in train_batcher.get_one_batch():\n",
    "#         print(train_idxs)\n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1002ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GNNlayer=GCNConv\n",
    "\n",
    "class LinkModel(nn.Module):\n",
    "    def __init__(self, input_rep, num_neurons=64):\n",
    "        super(LinkModel, self).__init__()\n",
    "        \n",
    "        self.MLP1 = nn.Linear(input_rep,num_neurons)        \n",
    "        #self.MLP2 = nn.Linear(num_neurons,num_neurons)\n",
    "        self.MLP3 = nn.Linear(num_neurons*2,1)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "                            \n",
    "        x = self.MLP1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        y = self.MLP1(y)\n",
    "        y = y.relu()\n",
    "        y = F.dropout(y, p=0.5, training=self.training)\n",
    "        \n",
    "        xy=torch.cat((x-y,x*y),dim=1)  #         xy=x+y\n",
    "        \n",
    "#         z = self.MLP2(xy)\n",
    "#         z = z.relu()\n",
    "#         z = F.dropout(z, p=0.5, training=self.training)\n",
    "\n",
    "        z = self.MLP3(xy)\n",
    "        \n",
    "#         z = torch.sigmoid(z)\n",
    "#         z = z.relu()\n",
    "\n",
    "        return z\n",
    "\n",
    "    def compute_loss(self,x,y, target):\n",
    "        \n",
    "        pred = self.forward(x,y)        \n",
    "        loss = F.cross_entropy(pred, target)                \n",
    "        \n",
    "        return loss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f5b0e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LinkModel(data.num_features, num_neurons=64).to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9c99faf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_false_non_edges=[]\n",
    "train_false_edges = []\n",
    "\n",
    "minibatch_size = args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e809c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, g_data, adj_mat, false_non_edges=[], false_edges=[], small_samples=100):    \n",
    "    \n",
    "#     print(g_data.y)\n",
    "    \n",
    "    g_data.to(device)\n",
    "    model.eval()   \n",
    "    \n",
    "    edges, non_edges = sample_equal_number_edges_non_edges(adj_mat, false_non_edges=false_non_edges, false_edges=false_edges, small_samples=small_samples)\n",
    "    \n",
    "#     print(edges, \" --- \" ,non_edges)\n",
    "    \n",
    "    edges = torch.LongTensor(edges)\n",
    "    non_edges = torch.LongTensor(non_edges)    \n",
    "    \n",
    "    edge_mask = (g_data.y[edges[:,0]]==g_data.y[edges[:,1]]).type(torch.float).to(device) \n",
    "    \n",
    "    if non_edges.dim() == 1:\n",
    "#         print(non_edges)\n",
    "        non_edge_mask = non_edges.type(torch.float)\n",
    "    else:\n",
    "        non_edge_mask = (g_data.y[non_edges[:,0]]==g_data.y[non_edges[:,1]]).type(torch.float)\n",
    "    \n",
    "    samples = edges       \n",
    "    target = edge_mask\n",
    "    \n",
    "#     print(samples)\n",
    "#     print(target)\n",
    "    \n",
    "#     samples = torch.cat((edges, non_edges), dim=0).to(device)        \n",
    "#     target = torch.cat((edge_mask, non_edge_mask),dim=0).type(torch.long).to(device)\n",
    "    \n",
    "    \n",
    "    #target = torch.cat((torch.ones(len(edges)), torch.zeros(len(non_edges))),dim=0).type(torch.long).to(device)\n",
    "    batcher = MiniBatcher(min(len(samples),minibatch_size), len(samples)) if minibatch_size > 0 else MiniBatcher(len(samples), len(samples))\n",
    "    \n",
    "    preds=np.array([])\n",
    "    targets=np.array([])\n",
    "    \n",
    "    \n",
    "    \n",
    "#     pbar = tqdm(total=len(samples))\n",
    "#     pbar.set_description(f'predicting: ')\n",
    "        \n",
    "    with torch.no_grad():                  \n",
    "    \n",
    "        for idxs in batcher.get_one_batch():\n",
    "\n",
    "            idxs = idxs.to(device)\n",
    "            test_edges=samples[idxs]\n",
    "            test_target=target[idxs]\n",
    "            \n",
    "            out = model(g_data.x[test_edges[:,0]],g_data.x[test_edges[:,1]])\n",
    "            \n",
    "#             print(test_edges)\n",
    "#             print(test_target)\n",
    "#             print(out)\n",
    "            \n",
    "            pred = torch.zeros_like(out)\n",
    "            pred[out >= 0.5] = 1\n",
    "            \n",
    "#             print(pred)\n",
    "                              \n",
    "#             pred = out.argmax(dim=1)\n",
    "            \n",
    "#             print(pred.shape, pred)\n",
    "#             print(test_target.shape, test_target)\n",
    "            \n",
    "            pred = pred.cpu().numpy()\n",
    "            test_target=test_target.cpu().numpy()\n",
    "        \n",
    "            preds = np.append(preds,pred)\n",
    "            targets = np.append(targets,test_target)\n",
    "\n",
    "#             pbar.update(len(idxs))\n",
    "\n",
    "#         pbar.close()\n",
    "    \n",
    "    micro=f1_score(targets, preds, average='micro')\n",
    "    weighted=f1_score(targets, preds, average='weighted')\n",
    "    acc=accuracy_score(targets, preds)\n",
    "    \n",
    "    return acc, micro, weighted\n",
    "\n",
    "# minibatch_size = 10\n",
    "# predict(model, G_train, adj_train, small_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "08c7daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, G_train, adj_train, selfloop = False, log = True, epochs=1, small_samples=100):    \n",
    "    \n",
    "    if log:\n",
    "        if selfloop:\n",
    "            print(\"SelfLoop used\")\n",
    "        else:\n",
    "            print(\"Selfloop is omitted for other data\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    #criterion = torch.nn.CrossEntropyLoss()\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    worker = 0     \n",
    "    train_losses=[]\n",
    "    \n",
    "    G_train.to(device)\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "        if selfloop:\n",
    "            edges, non_edges, identity_edges, edge_types = train_sample_equal_number_edges_non_edges(adj_train, \n",
    "                                                                   false_non_edges=train_false_non_edges, \n",
    "                                                                   false_edges=train_false_edges, \n",
    "                                                                   small_samples=small_samples, N = data.num_nodes)\n",
    "\n",
    "            edge_types = edge_types.to(device)\n",
    "            identity_edges = identity_edges.to(device)\n",
    "\n",
    "            edges = torch.LongTensor(edges).to(device)\n",
    "            non_edges = torch.LongTensor(non_edges).to(device)  \n",
    "\n",
    "            samples = torch.cat((edges, non_edges, identity_edges), dim=0).to(device)    \n",
    "            edge_mask = (G_train.y[edges[:,0]]==G_train.y[edges[:,1]]).type(torch.float)\n",
    "            #edge_mask = (2*edge_mask-1).to(device)\n",
    "#             print(edge_mask)\n",
    "            \n",
    "            if non_edges.dim() == 1:\n",
    "#                 print(non_edges)\n",
    "                non_edge_mask = non_edges.type(torch.float)\n",
    "            else:\n",
    "                #non_edge_mask = torch.zeros(len(non_edges)).to(device)\n",
    "                non_edge_mask = (G_train.y[non_edges[:,0]]==G_train.y[non_edges[:,1]]).type(torch.float).to(device)\n",
    "                \n",
    "            \n",
    "#             print(edges)\n",
    "#             print(identity_edges)\n",
    "#             print(non_edges)            \n",
    "#             print(identity_edges.shape)\n",
    "#             print(edge_mask.shape)\n",
    "#             print(edge_mask)\n",
    "#             print(non_edge_mask.shape)\n",
    "#             print(non_edge_mask)\n",
    "\n",
    "            identity_edges_mask = torch.ones(identity_edges.shape[0], dtype=torch.float).to(device)\n",
    "\n",
    "            target = torch.cat((edge_mask, non_edge_mask, identity_edges_mask),dim=0).to(device)\n",
    "\n",
    "            assert edge_types.shape[0] == target.shape[0]\n",
    "        else:\n",
    "            \n",
    "            edges, non_edges = sample_equal_number_edges_non_edges(adj_train, \n",
    "                                                                   false_non_edges=train_false_non_edges, \n",
    "                                                                   false_edges=train_false_edges, \n",
    "                                                                   small_samples=small_samples)\n",
    "\n",
    "            edges = torch.LongTensor(edges).to(device)\n",
    "            non_edges = torch.LongTensor(non_edges).to(device)  \n",
    "\n",
    "            samples = torch.cat((edges, non_edges), dim=0).to(device)    \n",
    "            edge_mask = (G_train.y[edges[:,0]]==G_train.y[edges[:,1]]).type(torch.float)\n",
    "            #edge_mask = (2*edge_mask-1).to(device)\n",
    "\n",
    "#             print(edge_mask)\n",
    "            \n",
    "            if non_edges.dim() == 1:\n",
    "#                 print(non_edges)\n",
    "                non_edge_mask = non_edges.type(torch.float)\n",
    "\n",
    "            else:                \n",
    "                #non_edge_mask = torch.zeros(len(non_edges)).to(device)\n",
    "                non_edge_mask = (G_train.y[non_edges[:,0]]==G_train.y[non_edges[:,1]]).type(torch.float).to(device)\n",
    "\n",
    "            target = torch.cat((edge_mask, non_edge_mask),dim=0).to(device)\n",
    "\n",
    "    #         samples = torch.cat((torch.Tensor(edges), torch.Tensor(non_edges)),dim=0).type(torch.long).to(device)\n",
    "    #         target = torch.cat((torch.ones(len(edges)), torch.zeros(len(non_edges))),dim=0).type(torch.long).to(device)\n",
    "        \n",
    "        train_batcher = MiniBatcher(min(len(samples),minibatch_size), len(samples)) if minibatch_size > 0 else MiniBatcher(len(samples), len(samples))\n",
    "    \n",
    "        total_loss = total_examples = 0\n",
    "        y_pred=[]\n",
    "        y_true=[]\n",
    "        \n",
    "#         pbar = tqdm(total=len(samples))\n",
    "#         pbar.set_description(f'Epoch {epoch:02d}')\n",
    "        \n",
    "        model.train()        \n",
    "        for train_idxs in train_batcher.get_one_batch():\n",
    "            \n",
    "            train_idxs = train_idxs.to(device)\n",
    "\n",
    "            if selfloop:\n",
    "            \n",
    "                train_edge_types = edge_types[train_idxs]\n",
    "                train_edges = samples[train_idxs]\n",
    "                train_target = target[train_idxs]\n",
    "\n",
    "                x = G_train.x[train_edges[~train_edge_types][:,0]]\n",
    "                y = G_train.x[train_edges[~train_edge_types][:,1]]\n",
    "                t = train_target[~train_edge_types]\n",
    "\n",
    "                x1 = data.x[train_edges[train_edge_types][:,0]].to(device)\n",
    "                y1 = data.x[train_edges[train_edge_types][:,1]].to(device)\n",
    "                t1 = train_target[train_edge_types]\n",
    "\n",
    "                x = torch.cat((x,x1),dim=0)\n",
    "                y = torch.cat((y,y1),dim=0)\n",
    "                train_target = torch.cat((t,t1),dim=0)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                out = model(x,y)\n",
    "                \n",
    "            else:\n",
    "                train_edges=samples[train_idxs]\n",
    "                train_target=target[train_idxs]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                out = model(G_train.x[train_edges[:,0]],G_train.x[train_edges[:,1]])\n",
    "\n",
    "            \n",
    "            #loss = F.nll_loss(out, train_target)\n",
    "            loss = criterion(out, train_target.view(-1,1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * len(train_idxs)\n",
    "            total_examples += len(train_idxs)\n",
    "        \n",
    "#             pbar.update(len(train_idxs))\n",
    "#         pbar.close()\n",
    "        \n",
    "        loss=total_loss / total_examples\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        if log: \n",
    "            print(f'Epoch {epoch:03d} Loss {loss:.4f}', end=' ')        \n",
    "            a,b,c = predict(model, G_train, adj_train, small_samples=minibatch_size)\n",
    "        if log:        \n",
    "            print(f'\\t{a:.4f},\\t{b:.4f},\\t{c:.4f}')\n",
    "        \n",
    "        \n",
    "    return model\n",
    "\n",
    "# train(model, data, G_train, adj_train, log = True, epochs=200, small_samples=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0f29a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(model, G_train, adj_train, small_samples=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "67d89c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predict(model, G_val, adj_validation, small_samples=1024))\n",
    "# print(predict(model, G_test, adj_test, small_samples=1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e23b6",
   "metadata": {},
   "source": [
    "# Train link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2251ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_link(data, selfloop = False, log=True):\n",
    "    \n",
    "    adj_mat = torch.zeros((data.num_nodes,data.num_nodes))\n",
    "    #edge_index = data.edge_index\n",
    "    edge_index, _ = add_self_loops(data.edge_index)            \n",
    "    edges = edge_index.t()\n",
    "    \n",
    "    adj_mat[edges[:,0], edges[:,1]] = 1\n",
    "    adj_mat[edges[:,1], edges[:,0]] = 1\n",
    "    \n",
    "    adj_train = adj_mat[data.train_mask].t()[data.train_mask].t()\n",
    "    adj_validation = adj_mat[data.val_mask].t()[data.val_mask].t()\n",
    "    adj_test = adj_mat[data.test_mask].t()[data.test_mask].t()\n",
    "    \n",
    "    G_train=Data(edge_index=(adj_train.nonzero()).t(), x=data.x[data.train_mask], y=data.y[data.train_mask])\n",
    "    G_val=Data(edge_index=(adj_validation.nonzero()).t(), x=data.x[data.val_mask], y=data.y[data.val_mask])\n",
    "    G_test=Data(edge_index=(adj_test.nonzero()).t(), x=data.x[data.test_mask], y=data.y[data.test_mask])\n",
    "    G_train\n",
    "    \n",
    "    model = LinkModel(data.num_features, num_neurons=args.num_neurons).to(device)\n",
    "    \n",
    "    if log:\n",
    "        print(model)\n",
    "    \n",
    "    if data.num_nodes>100000:\n",
    "        epochs = 5\n",
    "    else:\n",
    "        epochs = args.epochs\n",
    "    \n",
    "    model  = train(model, data, G_train, adj_train, selfloop, log, epochs=epochs, small_samples=minibatch_size)\n",
    "    \n",
    "    if log:\n",
    "        print(predict(model, G_train, adj_train, small_samples=minibatch_size))\n",
    "        print(predict(model, G_val, adj_validation, small_samples=minibatch_size))\n",
    "        print(predict(model, G_test, adj_test, small_samples=minibatch_size))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "aa5dc901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.epochs =150\n",
    "# data, dataset = get_data('karate', log=False)\n",
    "# model = train_link(data, selfloop = True, log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91971043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "05c87a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # link self attention is considered for training data only, test, validation node self attention is not covered\n",
    "# data, dataset = get_data('karate', log = False)\n",
    "# model = train_link(data, selfloop = True, log = True)\n",
    "\n",
    "# x = data.x\n",
    "# x_col1 = x.to(device)\n",
    "# x_col2 = x.to(device)\n",
    "# output = model(x_col1, x_col2)\n",
    "\n",
    "# print(output)\n",
    "# # output = output.softmax(dim=1)\n",
    "# # second_column = output[:,1].cpu()        \n",
    "# # print(second_column) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3684fa",
   "metadata": {},
   "source": [
    "## KNN Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ae7d866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPred():\n",
    "    \n",
    "    def __init__(self, data, selfloop = False, log=True):\n",
    "        \n",
    "#         edge_index, _ = add_self_loops(data.edge_index)        \n",
    "#         data.edge_index = edge_index\n",
    "        \n",
    "        self.N = N = data.num_nodes\n",
    "        self.E = E = data.num_edges\n",
    "        self.data = data\n",
    "        self.log = log\n",
    "        self.selfloop = selfloop\n",
    "        \n",
    "        self.adj = SparseTensor(\n",
    "            row=data.edge_index[0], col=data.edge_index[1],\n",
    "            value=torch.arange(E, device=data.edge_index.device),\n",
    "            sparse_sizes=(N, N))\n",
    "                \n",
    "        self.model = train_link(data, selfloop, log)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def lazy_greedy_weight(self,u):\n",
    "    \n",
    "        row, col, edge_index = self.adj[u,:].coo()   \n",
    "        \n",
    "        if len(col)==0:\n",
    "            return [],[]\n",
    "                \n",
    "#         print(self.data.x[u].repeat(len(col),1).shape)\n",
    "#         print(self.data.x[col.tolist()].shape)\n",
    "        \n",
    "        x = self.data.x[u].repeat(len(col),1).to(device)\n",
    "        y = self.data.x[col.tolist()].to(device)\n",
    "        \n",
    "        outs  = self.model(x, y)        \n",
    "#         outs = outs.softmax(dim=1)\n",
    "#         w = outs[:,1].cpu()        \n",
    "        w = outs.cpu().view(-1)\n",
    "        #w = torch.clamp(w.round()+0.01, max=1.0)        \n",
    "\n",
    "        S_G = w.tolist()\n",
    "        S_edge = edge_index.tolist()\n",
    "        \n",
    "        return S_G, S_edge\n",
    "\n",
    "    def get_submodular_weight(self):\n",
    "        \n",
    "        if self.log:\n",
    "            pbar = tqdm(total=self.N)\n",
    "            pbar.set_description(f'Nodes')\n",
    "\n",
    "        edge_weight=[]\n",
    "        edge_index=[]\n",
    "\n",
    "        for u in range(self.N):            \n",
    "            weight, e_index = self.lazy_greedy_weight(u)\n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            if self.log:\n",
    "                pbar.update(1)\n",
    "        if self.log:\n",
    "            pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E\n",
    "        \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)        \n",
    "        \n",
    "        return weight\n",
    "    \n",
    "    def process_block(self, list_u):\n",
    "        \n",
    "        #print(\"Processing :\",len(list_u), list_u[0], list_u[-1])\n",
    "        \n",
    "        edge_weight = []\n",
    "        edge_index = []\n",
    "        \n",
    "        for u in list_u:        \n",
    "            weight, e_index = self.lazy_greedy_weight(u)            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            \n",
    "        #print(\"Done :\",len(list_u), list_u[0], list_u[-1])\n",
    "            \n",
    "        return edge_weight, edge_index, len(list_u)\n",
    "    \n",
    "    #multiprocessing\n",
    "    def get_submodular_weight_multiproces(self):\n",
    "        \n",
    "        edge_weight=[]\n",
    "        edge_index=[]        \n",
    "        \n",
    "        N = self.N\n",
    "        #N = 1000\n",
    "        \n",
    "        elem_size=100\n",
    "        num_blocks = int(N/elem_size)\n",
    "        nodes = np.arange(num_blocks*elem_size).reshape(num_blocks,-1).tolist()\n",
    "        if num_blocks*elem_size<N:\n",
    "            nodes.append(list(range(num_blocks*elem_size,N)))        \n",
    "        \n",
    "        pool_size = NUM_PROCESSORS        \n",
    "        print(\"Pool Size: \", pool_size)        \n",
    "        pool = Pool(pool_size)\n",
    "                \n",
    "        pbar = tqdm(total=N)\n",
    "        pbar.set_description(f'Nodes')  \n",
    "                \n",
    "        for (weight, e_index, num_el) in pool.imap_unordered(self.process_block, nodes):            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            pbar.update(num_el)\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E        \n",
    "        \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)\n",
    "        \n",
    "        return weight\n",
    "    \n",
    "    \n",
    "    def compute_weights(self):\n",
    "        if isnotebook() or self.data.num_nodes<1000:\n",
    "            weight = self.get_submodular_weight()    \n",
    "        else:\n",
    "            weight = self.get_submodular_weight_multiproces()\n",
    "        \n",
    "        return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3a486d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, dataset = get_data('texas')\n",
    "# submodular_weight = LinkPred(data)\n",
    "# #submodular_weight.lazy_greedy_weight(3)\n",
    "# weight = submodular_weight.compute_weights()\n",
    "# weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "86aee8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "719cd483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(weight, 'Weights/moon_weight.pt')\n",
    "# torch.load('Weights/moon_weight.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6d20ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkNN():\n",
    "    \n",
    "    def __init__(self, data, value='min', log=True):\n",
    "        \n",
    "        self.N = N = data.num_nodes\n",
    "        self.E = E = data.num_edges\n",
    "        self.data = data        \n",
    "        self.value = value\n",
    "        self.log = log\n",
    "        \n",
    "        self.sign = 1\n",
    "        \n",
    "        if value=='min':\n",
    "            self.sign = -1\n",
    "            \n",
    "        self.adj = SparseTensor(\n",
    "            row=data.edge_index[0], col=data.edge_index[1],\n",
    "            value=torch.arange(E, device=data.edge_index.device),\n",
    "            sparse_sizes=(N, N))\n",
    "        \n",
    "    def lazy_greedy_weight(self,u):\n",
    "    \n",
    "        row, col, edge_index = self.adj[u,:].coo()           \n",
    "        \n",
    "        target_class_sim = self.data.weight[edge_index]\n",
    "        ind = np.argsort(self.sign*target_class_sim) #-1*desending, normal will be ascending\n",
    "        \n",
    "#         print(u, row, col, edge_index)\n",
    "#         print(target_class_sim)\n",
    "#         print(ind)\n",
    "         \n",
    "        lambda1 = 0.25 #top 25% with probability 1\n",
    "        lambda2 = 0.25 #second 25% with probability 0.5 \n",
    "        \n",
    "        l1=math.ceil(len(col)*lambda1)\n",
    "        l2=min(len(col)-l1,math.ceil(len(col)*lambda2))        \n",
    "        l3=max(0,int(len(col)-l1-l2))\n",
    "        \n",
    "#         print(len(col),l1, l2, l3)\n",
    "        \n",
    "#         S_G = np.ones(l1, dtype=float)*1.0\n",
    "#         S_G = np.append(S_G, np.ones(l2, dtype=float)*0.5)\n",
    "#         if(l3>0):\n",
    "#             S_G = np.append(S_G, np.ones(l3, dtype=float)*0.1)\n",
    "\n",
    "        S_G = np.ones(l1, dtype=float)*1.0\n",
    "        S_G = np.append(S_G, np.ones(l2, dtype=float)*0.5)\n",
    "        \n",
    "        if(l3>0):\n",
    "            S_G = np.append(S_G, np.ones(l3, dtype=float)*0.1)\n",
    "        \n",
    "        S_G = S_G.tolist()\n",
    "        \n",
    "#         S_G = list(range(1,len(col)+1))\n",
    "        S_edge = edge_index[ind].tolist()\n",
    "        \n",
    "        return S_G, S_edge\n",
    "\n",
    "    def get_submodular_weight(self):\n",
    "        \n",
    "        if self.log:        \n",
    "            pbar = tqdm(total=self.N)\n",
    "            pbar.set_description(f'Nodes')\n",
    "\n",
    "        edge_weight=[]\n",
    "        edge_index=[]\n",
    "\n",
    "        for u in range(self.N):            \n",
    "            weight, e_index = self.lazy_greedy_weight(u)\n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            if self.log:        \n",
    "                pbar.update(1)\n",
    "        if self.log:        \n",
    "            pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E\n",
    "        \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)\n",
    "        \n",
    "        return weight\n",
    "    \n",
    "    def process_block(self, list_u):\n",
    "        \n",
    "        #print(\"Processing :\",len(list_u), list_u[0], list_u[-1])\n",
    "        \n",
    "        edge_weight = []\n",
    "        edge_index = []\n",
    "        \n",
    "        for u in list_u:        \n",
    "            weight, e_index = self.lazy_greedy_weight(u)            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            \n",
    "        #print(\"Done :\",len(list_u), list_u[0], list_u[-1])\n",
    "            \n",
    "        return edge_weight, edge_index, len(list_u)\n",
    "    \n",
    "    #multiprocessing\n",
    "    def get_submodular_weight_multiproces(self):\n",
    "        \n",
    "        edge_weight=[]\n",
    "        edge_index=[]        \n",
    "        \n",
    "        N = self.N\n",
    "        #N = 1000\n",
    "        \n",
    "        elem_size=100\n",
    "        num_blocks = int(N/elem_size)\n",
    "        nodes = np.arange(num_blocks*elem_size).reshape(num_blocks,-1).tolist()\n",
    "        if num_blocks*elem_size<N:\n",
    "            nodes.append(list(range(num_blocks*elem_size,N)))        \n",
    "        \n",
    "        pool_size = NUM_PROCESSORS        \n",
    "        print(\"Pool Size: \", pool_size)        \n",
    "        pool = Pool(pool_size)\n",
    "                \n",
    "        pbar = tqdm(total=N)\n",
    "        pbar.set_description(f'Nodes')  \n",
    "                \n",
    "        for (weight, e_index, num_el) in pool.imap_unordered(self.process_block, nodes):            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            pbar.update(num_el)\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E        \n",
    "        \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)\n",
    "        \n",
    "        return weight\n",
    "    \n",
    "    \n",
    "    def compute_weights(self):\n",
    "        if isnotebook() or self.data.num_nodes<1000:\n",
    "            weight = self.get_submodular_weight()    \n",
    "        else:\n",
    "            weight = self.get_submodular_weight_multiproces()\n",
    "        \n",
    "        return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e450128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkSub():\n",
    "    \n",
    "    def __init__(self, data, value='max', selfloop = False, log = True):\n",
    "        \n",
    "        self.N = N = data.num_nodes\n",
    "        self.E = E = data.num_edges\n",
    "        self.data = data\n",
    "        self.log = log\n",
    "        self.selfloop = selfloop\n",
    "        \n",
    "        self.X = data.x.to(device)\n",
    "        \n",
    "        self.model = train_link(data, selfloop = selfloop, log= log)\n",
    "        self.model.eval()        \n",
    "\n",
    "        self.adj = SparseTensor(\n",
    "            row=data.edge_index[0], col=data.edge_index[1],\n",
    "            value=torch.arange(E, device=data.edge_index.device),\n",
    "            sparse_sizes=(N, N))\n",
    "        \n",
    "        if self.log:\n",
    "            print(\"value: \", value)\n",
    "        \n",
    "        self.value = value\n",
    "        self.sign = -1\n",
    "        \n",
    "        if self.value == 'max':\n",
    "            self.sign = 1 ##-1 select the nearest ones, 1 for the farthest        \n",
    "            \n",
    "        elif self.value == 'min':\n",
    "            self.sign = -1\n",
    "        else:\n",
    "            raise 'Not implemented error'\n",
    "    \n",
    "    def pairwise_link(self, x):  \n",
    "                \n",
    "        n, f = x.shape\n",
    "        \n",
    "        x_col1 = x.repeat_interleave(n, dim=0)\n",
    "        x_col2 = x.repeat(n,1)\n",
    "        # print(x_col1, x_col2)\n",
    "        \n",
    "        output = self.model(x_col1, x_col2).cpu()\n",
    "        #print(output.shape)\n",
    "\n",
    "#         output = output.softmax(dim=1)\n",
    "#         second_column = output[:,1].cpu()        \n",
    "        #print(second_column)\n",
    "        \n",
    "        similarity_matrix = output.view(n,n)\n",
    "        \n",
    "#         print(similarity_matrix)\n",
    "        \n",
    "        return similarity_matrix\n",
    "        \n",
    "    def lazy_greedy_weight(self,u):\n",
    "        \n",
    "        row, col, edge_index = self.adj[u,:].coo()\n",
    "        vertices = [u]+col.tolist()\n",
    "        \n",
    "        v2i={i:j for i,j in zip(vertices, range(len(vertices)))}\n",
    "        i2v={value:key for key, value in v2i.items()}\n",
    "        \n",
    "        kernel_dist = self.pairwise_link(self.X[vertices])\n",
    "        \n",
    "        gain_list=[(self.sign*kernel_dist[v2i[u],v2i[v.item()]],v.item(), e.item()) for v,e in zip(col,edge_index)] \n",
    "        #-1 selecting nearest\n",
    "        #1 selecting farthest\n",
    "\n",
    "        heapq.heapify(gain_list)\n",
    "        #print(gain_list)\n",
    "\n",
    "        S=[u]\n",
    "        S_G=[]\n",
    "        S_edge=[]\n",
    "        S_index=[v2i[u]]\n",
    "        \n",
    "        lambda1 = 0.25 #top 25% with probability 1\n",
    "        lambda2 = 0.25 #second 25% with probability 0.5         \n",
    "        l1=math.ceil(len(col)*lambda1)\n",
    "        l2=min(len(col)-l1,math.ceil(len(col)*lambda2))\n",
    "        l3=max(0,int(len(col)-l1-l2))\n",
    "        \n",
    "        #print(len(col),l1, l2, l3)\n",
    "        \n",
    "        rank=1 #rank weight instead gain weight\n",
    "        \n",
    "        while(gain_list):\n",
    "            (gain_v, v, e) = heapq.heappop(gain_list)\n",
    "            gain_v = self.sign*gain_v #make it positive\n",
    "            #print(gain_v, v)\n",
    "\n",
    "            if len(gain_list)==0:\n",
    "                if gain_v<1e-6:\n",
    "                    gain_v=1e-6\n",
    "                S.append(v)\n",
    "                #S_G.append(gain_v)\n",
    "                #S_G.append(rank)\n",
    "                \n",
    "                if rank <= l1:\n",
    "                    S_G.append(1.0)\n",
    "                \n",
    "                elif rank<=l1+l2:\n",
    "                    S_G.append(0.5)\n",
    "                \n",
    "                else:\n",
    "                    S_G.append(0.1)\n",
    "                \n",
    "                rank+=1\n",
    "                \n",
    "                S_edge.append(e)\n",
    "                S_index.append(v2i[v])\n",
    "                \n",
    "                break\n",
    "            \n",
    "            gain_v_update = self.sign*min(kernel_dist[v2i[v],S_index])\n",
    "            \n",
    "            #print(\"updated: \", S,v,gain_v_update, gain_v)\n",
    "            (gain_v_second,v_second,_)=gain_list[0] #top\n",
    "            gain_v_second = gain_v_second #make it positive\n",
    "\n",
    "            if gain_v_update<=gain_v_second:\n",
    "                \n",
    "                gain_v_update = self.sign*gain_v_update\n",
    "                \n",
    "                if gain_v_update<1e-6:\n",
    "                    gain_v_update=1e-6\n",
    "                S.append(v)\n",
    "                #S_G.append(gain_v_update)\n",
    "                #S_G.append(rank)\n",
    "                \n",
    "                if rank<=l1:\n",
    "                    S_G.append(1.0)\n",
    "                elif rank<=l1+l2:\n",
    "                    S_G.append(0.5)\n",
    "                else:\n",
    "                    S_G.append(0.1)\n",
    "                rank+=1\n",
    "                \n",
    "                S_edge.append(e)\n",
    "                S_index.append(v2i[v])\n",
    "            else:\n",
    "                heapq.heappush(gain_list,(self.sign*gain_v_update,v, e))\n",
    "\n",
    "        return S_G, S_edge\n",
    "    \n",
    "    #serial\n",
    "    def get_submodular_weight(self):\n",
    "        \n",
    "        N = self.N\n",
    "        #N = 1000\n",
    "        \n",
    "        if self.log:\n",
    "            pbar = tqdm(total=N)\n",
    "            pbar.set_description(f'Nodes')\n",
    "\n",
    "        edge_weight=[]\n",
    "        edge_index=[]\n",
    "        \n",
    "        test = 0\n",
    "\n",
    "        for u in range(N):                \n",
    "            weight, e_index = self.lazy_greedy_weight(u)\n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "        \n",
    "            #test += sum((np.array(weight)>1.0).astype(int))\n",
    "            if self.log:\n",
    "                pbar.update(1)\n",
    "        \n",
    "        #print(test)\n",
    "        if self.log:\n",
    "            pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E        \n",
    "        \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)\n",
    "        \n",
    "        return weight\n",
    "        \n",
    "    \n",
    "    def process_block(self, list_u):\n",
    "        \n",
    "        #print(\"Processing :\",len(list_u), list_u[0], list_u[-1])\n",
    "        \n",
    "        edge_weight = []\n",
    "        edge_index = []\n",
    "        \n",
    "        for u in list_u:        \n",
    "            weight, e_index = self.lazy_greedy_weight(u)            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            \n",
    "        #print(\"Done :\",len(list_u), list_u[0], list_u[-1])\n",
    "            \n",
    "        return edge_weight, edge_index, len(list_u)\n",
    "        \n",
    "    \n",
    "    #multiprocessing\n",
    "    def get_submodular_weight_multiproces(self):\n",
    "        \n",
    "        edge_weight=[]\n",
    "        edge_index=[]        \n",
    "        \n",
    "        N = self.N\n",
    "        #N = 1000\n",
    "        \n",
    "        elem_size=10\n",
    "        num_blocks = int(N/elem_size)\n",
    "        nodes = np.arange(num_blocks*elem_size).reshape(num_blocks,-1).tolist()\n",
    "        if num_blocks*elem_size<N:\n",
    "            nodes.append(list(range(num_blocks*elem_size,N)))        \n",
    "        \n",
    "        pool_size = NUM_PROCESSORS        \n",
    "        print(\"Pool Size: \", pool_size)        \n",
    "        pool = Pool(pool_size)\n",
    "        \n",
    "        if self.log:\n",
    "            pbar = tqdm(total=N)\n",
    "            pbar.set_description(f'Nodes')  \n",
    "                \n",
    "        for (weight, e_index, num_el) in pool.imap_unordered(self.process_block, nodes):            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            \n",
    "            if self.log:\n",
    "                pbar.update(num_el)\n",
    "        \n",
    "        if self.log:\n",
    "            pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E\n",
    "                \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)        \n",
    "        \n",
    "        return weight\n",
    "    \n",
    "    \n",
    "    def compute_weights(self):\n",
    "        \n",
    "        if isnotebook() or self.data.num_nodes<1000:\n",
    "            weight = self.get_submodular_weight()    \n",
    "        else:\n",
    "            weight = self.get_submodular_weight_multiproces()\n",
    "        \n",
    "        return weight\n",
    "    \n",
    "# data, dataset = get_data('karate', log=False)\n",
    "# submodular_weight = LinkSub(data, selfloop = False, log = True)\n",
    "# submodular_weight.lazy_greedy_weight(1)\n",
    "# #data.weight = submodular_weight.compute_weights()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6c8e1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, dataset = get_data('karate', log = False)\n",
    "# submodular_weight = LinkSub(data, log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9d24d2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submodular_weight.lazy_greedy_weight(0)\n",
    "#data.weight = submodular_weight.compute_weights()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "64f167c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = data.x\n",
    "# # x = torch.Tensor([[1,2],[3,4],[5,6]])\n",
    "# #print(x)\n",
    "# N, F = x.shape\n",
    "\n",
    "# # x_col1 = x.to(device)\n",
    "# # x_col2 = x.to(device)\n",
    "\n",
    "# x_col1 = x.repeat_interleave(N, dim=0).to(device)\n",
    "# x_col2 = x.repeat(N,1).to(device)\n",
    "# # print(x_col1, x_col2)\n",
    "\n",
    "# output = submodular_weight.model(x_col1, x_col2)\n",
    "# print(output.shape)\n",
    "\n",
    "# output = output.softmax(dim=1)\n",
    "# second_column = output[:,1].cpu()        \n",
    "# print(second_column)\n",
    "\n",
    "# # similarity_matrix = second_column.view(N,N)\n",
    "# # print(similarity_matrix)\n",
    "# # print(similarity_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddbb922",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "27c70d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipynb.fs.full.Dataset import pearson_coff\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def normalize_rows(arr):\n",
    "    row_min = arr.min(axis=1, keepdims=True)\n",
    "    row_max = arr.max(axis=1, keepdims=True)\n",
    "    return (arr - row_min) / (row_max - row_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c177d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c8a78c2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def link_correlation(DATASET_NAME):\n",
    "    data, dataset = get_data(DATASET_NAME, log=False, h_score=True, split_no = 0)\n",
    "    #data.train_mask = data.train_mask|data.val_mask|data.test_mask\n",
    "    #data.train_mask = data.train_mask|data.val_mask\n",
    "    args.epochs = 150\n",
    "    args.num_neuron = 32\n",
    "    #args.batch_size = 4096\n",
    "    \n",
    "    link_model = train_link(data, selfloop = True, log = True)\n",
    "    \n",
    "    feature_matrix = data.x.to(device)\n",
    "    N = feature_matrix.shape[0]\n",
    "    A = np.zeros((N,N))\n",
    "\n",
    "    print(N)\n",
    "\n",
    "    pbar = tqdm(total=N)\n",
    "    pbar.set_description(f'Nodes')\n",
    "\n",
    "    link_model.eval()\n",
    "    with torch.no_grad():    \n",
    "        for i in range(N):\n",
    "            x = feature_matrix[i].repeat(N, 1)\n",
    "            sim = link_model(x, feature_matrix)            \n",
    "            \n",
    "            pred = sim                        \n",
    "#             pred = torch.zeros_like(sim)\n",
    "#             pred[sim >= 0.5] = 1            \n",
    "            pred = pred.cpu().numpy().reshape(-1)            \n",
    "            A[i] = pred\n",
    "            pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    \n",
    "    print(A)\n",
    "#     A = normalize_rows(A)\n",
    "#     print(A)\n",
    "    \n",
    "#     for i in range(N):\n",
    "#         for j in range(N):\n",
    "#             print(f'{A[i,j]:0.2f}',end=' ')\n",
    "#         print(\"\")\n",
    "    \n",
    "    similarity_scores = A\n",
    "    labels = data.y\n",
    "    features = data.x\n",
    "\n",
    "    similarity_values = []\n",
    "    label_matches = []\n",
    "\n",
    "    pbar = tqdm(total=N)\n",
    "    pbar.set_description(f'Nodes')\n",
    "    # Compute similarity values and label matches\n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1, len(features)):\n",
    "            similarity_values.append(similarity_scores[i, j])\n",
    "            label_matches.append(int(labels[i] == labels[j]))\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    \n",
    "    \n",
    "    # Calculate Pearson's correlation coefficient\n",
    "    correlation, _ = pearsonr(similarity_values, label_matches)\n",
    "\n",
    "    print(correlation)\n",
    "    \n",
    "#     # Plot the correlation\n",
    "#     plt.scatter(similarity_values, label_matches, alpha=0.5)\n",
    "#     plt.title(f'Pearson Correlation: {correlation:.2f}')\n",
    "#     plt.xlabel('Feature Similarity')\n",
    "#     plt.ylabel('Label Match (1 if same, 0 if different)')\n",
    "#     plt.show()\n",
    "\n",
    "# link_correlation('karate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ec49c506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:  /scratch/gilbreth/das90/Dataset/\n",
      "Result directory: /scratch/gilbreth/das90/Dataset/RESULTS/\n",
      "\n",
      "Dataset: KarateClub():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 34\n",
      "Number of classes: 4\n",
      "\n",
      "Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34], val_mask=[34], test_mask=[34])\n",
      "===========================================================================================================\n",
      "Number of nodes: 34\n",
      "Number of edges: 156\n",
      "Average node degree: 4.59\n",
      "Number of training nodes: 4\n",
      "Training node label rate: 0.12\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n",
      "N  34  E  156  d  4.588235294117647 0.8020520210266113 0.7564102411270142 0.6170591711997986 -0.4756128787994385 LinkModel(\n",
      "  (MLP1): Linear(in_features=34, out_features=64, bias=True)\n",
      "  (MLP3): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "SelfLoop used\n",
      "Epoch 001 Loss 0.2928 \t0.6250,\t0.6250,\t0.4808\n",
      "Epoch 002 Loss 0.3933 \t0.6250,\t0.6250,\t0.4808\n",
      "Epoch 003 Loss 0.2750 \t0.6250,\t0.6250,\t0.4808\n",
      "Epoch 004 Loss 0.3122 \t0.5000,\t0.5000,\t0.3333\n",
      "Epoch 005 Loss 0.3192 \t0.5000,\t0.5000,\t0.3333\n",
      "Epoch 006 Loss 0.3582 \t0.1250,\t0.1250,\t0.0278\n",
      "Epoch 007 Loss 0.2247 \t0.6250,\t0.6250,\t0.4808\n",
      "Epoch 008 Loss 0.2406 \t0.6250,\t0.6250,\t0.4808\n",
      "Epoch 009 Loss 0.2815 \t0.6250,\t0.6250,\t0.4808\n",
      "Epoch 010 Loss 0.2091 \t0.7500,\t0.7500,\t0.6429\n",
      "Epoch 011 Loss 0.3427 \t0.3750,\t0.3750,\t0.2045\n",
      "Epoch 012 Loss 0.3125 \t0.1250,\t0.1250,\t0.0278\n",
      "Epoch 013 Loss 0.1948 \t0.7500,\t0.7500,\t0.6429\n",
      "Epoch 014 Loss 0.1286 \t0.3750,\t0.3750,\t0.2045\n",
      "Epoch 015 Loss 0.2261 \t0.2500,\t0.2500,\t0.1000\n",
      "Epoch 016 Loss 0.3202 \t0.8750,\t0.8750,\t0.8167\n",
      "Epoch 017 Loss 0.3656 \t0.5000,\t0.5000,\t0.3333\n",
      "Epoch 018 Loss 0.2638 \t0.3750,\t0.3750,\t0.2045\n",
      "Epoch 019 Loss 0.2717 \t0.2500,\t0.2500,\t0.1000\n",
      "Epoch 020 Loss 0.2258 \t0.5000,\t0.5000,\t0.3333\n",
      "Epoch 021 Loss 0.2171 \t0.5000,\t0.5000,\t0.3333\n",
      "Epoch 022 Loss 0.2352 \t0.5000,\t0.5000,\t0.3333\n",
      "Epoch 023 Loss 0.1225 \t0.3750,\t0.3750,\t0.2045\n",
      "Epoch 024 Loss 0.2247 \t0.6250,\t0.6250,\t0.4808\n",
      "Epoch 025 Loss 0.2346 \t0.2500,\t0.2500,\t0.1000\n",
      "Epoch 026 Loss 0.2164 \t0.5000,\t0.5000,\t0.3333\n",
      "Epoch 027 Loss 0.2855 \t0.5000,\t0.5000,\t0.3333\n",
      "Epoch 028 Loss 0.1888 \t0.3750,\t0.3750,\t0.2045\n",
      "Epoch 029 Loss 0.2350 \t0.5000,\t0.5000,\t0.3333\n",
      "Epoch 030 Loss 0.2105 \t0.2500,\t0.2500,\t0.1000\n",
      "Epoch 031 Loss 0.2296 \t0.3750,\t0.3750,\t0.2045\n",
      "Epoch 032 Loss 0.2513 \t0.3750,\t0.3750,\t0.2045\n",
      "Epoch 033 Loss 0.2734 \t0.7500,\t0.7500,\t0.6429\n",
      "Epoch 034 Loss 0.1963 \t0.5000,\t0.5000,\t0.3333\n",
      "Epoch 035 Loss 0.1858 \t0.5000,\t0.5000,\t0.3333\n",
      "Epoch 036 Loss 0.1989 \t0.5000,\t0.5000,\t0.3333\n",
      "Epoch 037 Loss 0.2196 \t0.5000,\t0.5000,\t0.3333\n",
      "Epoch 038 Loss 0.2237 \t0.3750,\t0.3750,\t0.4246\n",
      "Epoch 039 Loss 0.1835 \t0.7500,\t0.7500,\t0.7083\n",
      "Epoch 040 Loss 0.2149 \t0.6250,\t0.6250,\t0.4808\n",
      "Epoch 041 Loss 0.1235 \t0.5000,\t0.5000,\t0.5000\n",
      "Epoch 042 Loss 0.2118 \t0.5000,\t0.5000,\t0.3333\n",
      "Epoch 043 Loss 0.2453 \t0.5000,\t0.5000,\t0.4333\n",
      "Epoch 044 Loss 0.1367 \t0.7500,\t0.7500,\t0.7083\n",
      "Epoch 045 Loss 0.1295 \t0.5000,\t0.5000,\t0.3333\n",
      "Epoch 046 Loss 0.1848 \t0.3750,\t0.3750,\t0.2045\n",
      "Epoch 047 Loss 0.2055 \t0.5000,\t0.5000,\t0.3333\n",
      "Epoch 048 Loss 0.1882 \t0.7500,\t0.7500,\t0.7083\n",
      "Epoch 049 Loss 0.2048 \t0.6250,\t0.6250,\t0.5636\n",
      "Epoch 050 Loss 0.1514 \t0.7500,\t0.7500,\t0.7083\n",
      "Epoch 051 Loss 0.1709 \t0.6250,\t0.6250,\t0.4808\n",
      "Epoch 052 Loss 0.1783 \t0.6250,\t0.6250,\t0.5636\n",
      "Epoch 053 Loss 0.2021 \t0.5000,\t0.5000,\t0.4333\n",
      "Epoch 054 Loss 0.2043 \t0.7500,\t0.7500,\t0.6429\n",
      "Epoch 055 Loss 0.2361 \t0.2500,\t0.2500,\t0.2500\n",
      "Epoch 056 Loss 0.2040 \t0.7500,\t0.7500,\t0.7500\n",
      "Epoch 057 Loss 0.1620 \t0.5000,\t0.5000,\t0.5000\n",
      "Epoch 058 Loss 0.1176 \t0.5000,\t0.5000,\t0.3333\n",
      "Epoch 059 Loss 0.1020 \t0.6250,\t0.6250,\t0.4808\n",
      "Epoch 060 Loss 0.1223 \t0.6250,\t0.6250,\t0.5636\n",
      "Epoch 061 Loss 0.1455 \t0.6250,\t0.6250,\t0.6429\n",
      "Epoch 062 Loss 0.1572 \t0.7500,\t0.7500,\t0.7500\n",
      "Epoch 063 Loss 0.1664 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 064 Loss 0.2444 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 065 Loss 0.1832 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 066 Loss 0.1682 \t0.8750,\t0.8750,\t0.8682\n",
      "Epoch 067 Loss 0.2424 \t0.8750,\t0.8750,\t0.8770\n",
      "Epoch 068 Loss 0.1542 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 069 Loss 0.1380 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 070 Loss 0.1043 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 071 Loss 0.0929 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 072 Loss 0.0999 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 073 Loss 0.0965 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 074 Loss 0.1368 \t0.6250,\t0.6250,\t0.6429\n",
      "Epoch 075 Loss 0.1207 \t0.6250,\t0.6250,\t0.6071\n",
      "Epoch 076 Loss 0.1124 \t0.7500,\t0.7500,\t0.7917\n",
      "Epoch 077 Loss 0.1715 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 078 Loss 0.1157 \t0.8750,\t0.8750,\t0.8682\n",
      "Epoch 079 Loss 0.0949 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 080 Loss 0.1210 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 081 Loss 0.0957 \t0.7500,\t0.7500,\t0.7333\n",
      "Epoch 082 Loss 0.1014 \t0.8750,\t0.8750,\t0.8818\n",
      "Epoch 083 Loss 0.1258 \t0.8750,\t0.8750,\t0.8770\n",
      "Epoch 084 Loss 0.1283 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 085 Loss 0.1013 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 086 Loss 0.1033 \t0.8750,\t0.8750,\t0.8910\n",
      "Epoch 087 Loss 0.0897 \t0.8750,\t0.8750,\t0.8167\n",
      "Epoch 088 Loss 0.0846 \t0.6250,\t0.6250,\t0.6071\n",
      "Epoch 089 Loss 0.0890 \t0.7500,\t0.7500,\t0.6429\n",
      "Epoch 090 Loss 0.1343 \t0.8750,\t0.8750,\t0.8770\n",
      "Epoch 091 Loss 0.1039 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 092 Loss 0.0372 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 093 Loss 0.0870 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 094 Loss 0.1125 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 095 Loss 0.0294 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 096 Loss 0.0502 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 097 Loss 0.0908 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 098 Loss 0.1183 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 099 Loss 0.1405 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 100 Loss 0.1393 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 101 Loss 0.0821 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 102 Loss 0.0520 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 103 Loss 0.1342 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 104 Loss 0.1672 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 105 Loss 0.1502 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 106 Loss 0.0652 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 107 Loss 0.0537 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 108 Loss 0.0636 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 109 Loss 0.0592 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 110 Loss 0.1324 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 111 Loss 0.1320 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 112 Loss 0.1062 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 113 Loss 0.1217 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 114 Loss 0.0419 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 115 Loss 0.0643 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 116 Loss 0.0460 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 117 Loss 0.0579 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 118 Loss 0.0700 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 119 Loss 0.0891 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 120 Loss 0.0370 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 121 Loss 0.0752 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 122 Loss 0.0748 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 123 Loss 0.0568 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 124 Loss 0.0790 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 125 Loss 0.0206 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 126 Loss 0.1365 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 127 Loss 0.0970 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 128 Loss 0.1347 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 129 Loss 0.0628 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 130 Loss 0.0568 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 131 Loss 0.0911 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 132 Loss 0.0588 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 133 Loss 0.1447 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 134 Loss 0.0365 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 135 Loss 0.0965 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 136 Loss 0.0868 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 137 Loss 0.0676 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 138 Loss 0.0863 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 139 Loss 0.0704 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 140 Loss 0.0851 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 141 Loss 0.0536 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 142 Loss 0.0620 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 143 Loss 0.0620 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 144 Loss 0.0313 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 145 Loss 0.0747 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 146 Loss 0.1335 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 147 Loss 0.0526 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 148 Loss 0.1287 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 149 Loss 0.0560 \t1.0000,\t1.0000,\t1.0000\n",
      "Epoch 150 Loss 0.0405 \t1.0000,\t1.0000,\t1.0000\n",
      "(1.0, 1.0, 1.0)\n",
      "(0.8897058823529411, 0.8897058823529411, 0.8377775234607462)\n",
      "(0.8602941176470589, 0.8602941176470589, 0.7956870495233667)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nodes: 100%|| 34/34 [00:00<00:00, 2536.85it/s]\n",
      "Nodes: 100%|| 34/34 [00:00<00:00, 4290.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time:  0.6385443210601807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "    \n",
    "    log = True\n",
    "    \n",
    "    datasetname = args.dataset\n",
    "    \n",
    "    data, dataset = get_data(datasetname, log=log, h_score=True)\n",
    "#     data = generate_synthetic(data, d=100, h=0.25, train=0.1, random_state=1, log=log)\n",
    "    \n",
    "    start = time.time() \n",
    "    submodular_weight = LinkPred(data, selfloop = True, log = log)\n",
    "    data.weight = submodular_weight.compute_weights()    \n",
    "    submodular_weight = LinkNN(data, value ='min', log = log) \n",
    "    data.weight = submodular_weight.compute_weights()    \n",
    "    end = time.time()\n",
    "    print(\"Execution time: \", end-start)\n",
    "\n",
    "\n",
    "#     start = time.time()    \n",
    "#     submodular_weight = LinkSub(data, value ='max', selfloop = True, log = log)    \n",
    "#     data.weight = submodular_weight.compute_weights()    \n",
    "#     end = time.time()\n",
    "#     print(\"Execution time: \", end-start)\n",
    "    \n",
    "#     if 'weight' in data:\n",
    "#         cp_data= copy.deepcopy(data)\n",
    "#         G = to_networkx(cp_data, to_undirected=True, edge_attrs=['weight'])\n",
    "#         to_remove = [(a,b) for a, b, attrs in G.edges(data=True) if attrs[\"weight\"] <1.0 ]\n",
    "#         G.remove_edges_from(to_remove)\n",
    "#         updated_data = from_networkx(G)\n",
    "        \n",
    "#         print(updated_data)\n",
    "        \n",
    "#         updated_data = from_networkx(G, group_edge_attrs=['weight'])\n",
    "#         updated_data.weight = updated_data.edge_attr.view(-1)\n",
    "        \n",
    "#         row, col = updated_data.edge_index\n",
    "#         updated_data.edge_index = torch.stack((torch.cat((row, col),dim=0), torch.cat((col, row),dim=0)),dim=0)\n",
    "#         updated_data.weight = torch.cat((updated_data.weight, updated_data.weight),dim=0)\n",
    "        \n",
    "\n",
    "#         print(\"Node Homophily:\", homophily(updated_data.edge_index, cp_data.y, method='node'))\n",
    "#         print(\"Edge Homophily:\", homophily(updated_data.edge_index, cp_data.y, method='edge'))\n",
    "#         print(\"Edge_insensitive Homophily:\", homophily(updated_data.edge_index, cp_data.y, method='edge_insensitive'))    \n",
    "        \n",
    "    \n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "470305f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(weight, 'Weights/link_weight.pt')\n",
    "#torch.load('Weights/link_weight.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d1f0ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, dataset = get_data('Cora', log=False)\n",
    "# #     data = generate_synthetic(data, d=5, h=0.25, train=0.1, random_state=None, log=log)\n",
    "# submodular_weight = LinkSub(data, value ='min', selfloop = True, log = True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fa9e1566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submodular_weight.lazy_greedy_weight(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3092d167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
