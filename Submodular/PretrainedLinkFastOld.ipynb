{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bffbb0c",
   "metadata": {},
   "source": [
    "# Test Dataset to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38a1e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#as it turned out interactive shell (like Jupyter cannot handle CPU multiprocessing well so check which medium the code is runing)\n",
    "#we will write code in Jupyter for understanding purposes but final execuation will be in shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd880dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c466d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b785488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Utils import isnotebook\n",
    "from ipynb.fs.full.Dataset import get_data, generate_synthetic\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "import torch_geometric.utils.homophily as homophily\n",
    "import copy\n",
    "import ipynb.fs.full.utils.MoonGraph as MoonGraph\n",
    "import logging\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torch_geometric.utils import add_self_loops\n",
    "import heapq\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ebdc9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "from ipynb.fs.full.Dataset import datasets as available_datasets\n",
    "\n",
    "#set default arguments here\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()    \n",
    "    parser.add_argument('--log_info', type=bool, default=True)\n",
    "    parser.add_argument('--pbar', type=bool, default=False)\n",
    "    parser.add_argument('--balance', type=bool, default=True)\n",
    "    parser.add_argument('--num_worker', type=int, default=0)\n",
    "    parser.add_argument('--dataset', type=str, default=\"karate\", choices=available_datasets)\n",
    "    parser.add_argument('--epochs', type=int, default=20)\n",
    "    parser.add_argument('--link_batch_size', type=int, default=4096*8) #8192\n",
    "    parser.add_argument('--link_num_steps', type=int, default=100)\n",
    "    parser.add_argument('--num_neurons', type=int, default=32)\n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ae56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e61e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_sparse import SparseTensor\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "random.seed(12345)\n",
    "import numpy as np\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b0d46b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from multiprocessing.pool import ThreadPool, Pool\n",
    "import os.path as osp\n",
    "from typing import Optional, List, Dict\n",
    "from torch_sparse import SparseTensor\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.typing import EdgeType, InputNodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54170485",
   "metadata": {},
   "source": [
    "# Link Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ebe5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GINConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1002ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GNNlayer=GCNConv\n",
    "\n",
    "class LinkModel(nn.Module):\n",
    "    def __init__(self, input_rep, num_neurons=64):\n",
    "        super(LinkModel, self).__init__()\n",
    "        \n",
    "        self.MLP1 = nn.Linear(input_rep,num_neurons)        \n",
    "        #self.MLP2 = nn.Linear(num_neurons,num_neurons)\n",
    "        self.MLP3 = nn.Linear(num_neurons*2,1)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "                            \n",
    "        x = self.MLP1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        y = self.MLP1(y)\n",
    "        y = y.relu()\n",
    "        y = F.dropout(y, p=0.2, training=self.training)\n",
    "        \n",
    "        z=torch.cat((x-y,x*y),dim=1)  #         xy=x+y        \n",
    "#         z = self.MLP2(z)\n",
    "#         z = z.relu()\n",
    "#         z = F.dropout(z, p=0.5, training=self.training)\n",
    "\n",
    "        z = self.MLP3(z)\n",
    "#         z = torch.sigmoid(z)\n",
    "#         z = z.relu()\n",
    "#         z = F.log_softmax(z,dim=1)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5b0e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LinkModel(data.num_features, num_neurons=64).to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f13a430",
   "metadata": {},
   "source": [
    "## Link Prediction Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c9d609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkSampler(torch.utils.data.DataLoader):\n",
    "\n",
    "    def __init__(self, data, input_nodes: InputNodes = None, batch_size: int=1, num_steps: int = 1, \n",
    "                 save_dir: Optional[str] = None, recompute = True,log: bool = True, balance=False, **kwargs):\n",
    "\n",
    "        if 'collate_fn' in kwargs:\n",
    "            del kwargs['collate_fn']\n",
    "            \n",
    "        self.num_steps = num_steps\n",
    "        self.__batch_size__ = batch_size        \n",
    "        self.save_dir = save_dir\n",
    "        self.recompute = recompute\n",
    "        self.log = log\n",
    "        self.balance = balance\n",
    "        \n",
    "        self.data = data\n",
    "        \n",
    "        self.N = N = data.num_nodes\n",
    "        self.E = E = data.num_edges\n",
    "        \n",
    "        self.input_nodeidx = torch.nonzero(input_nodes).flatten()\n",
    "        \n",
    "        #print(self.input_nodeidx)\n",
    "        \n",
    "        if balance:\n",
    "            #get an estimate of ratio\n",
    "            indices = torch.randint(len(self.input_nodeidx), (self.__batch_size__, ))\n",
    "            x = self.input_nodeidx[indices]\n",
    "            indices = torch.randint(len(self.input_nodeidx), (self.__batch_size__, ))\n",
    "            y = self.input_nodeidx[indices]\n",
    "            label = (self.data.y[x] == self.data.y[y]).type(torch.float)\n",
    "            self.ratio = torch.sum(label).item()/self.__batch_size__\n",
    "            \n",
    "            #print(self.ratio)\n",
    "                        \n",
    "            #######\n",
    "            self.num_class = torch.max(data.y)+1        \n",
    "            self.clusters = [[] for i in range(self.num_class)]\n",
    "        \n",
    "            for i in self.input_nodeidx:\n",
    "                self.clusters[data.y[i]].append(i.item())\n",
    "                \n",
    "            for i in range(self.num_class):\n",
    "                self.clusters[i] = torch.LongTensor(self.clusters[i])\n",
    "            \n",
    "            #print(self.clusters)\n",
    "        \n",
    "\n",
    "        super().__init__(self, batch_size=1, collate_fn=self.__collate__,\n",
    "                         **kwargs)\n",
    "    \n",
    "    @property\n",
    "    def __filename__(self):\n",
    "        return f'{self.__class__.__name__.lower()}_{self.sample_coverage}.pt'\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_steps\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.balance and self.ratio<=0.40:\n",
    "            \n",
    "            per_class = math.ceil(self.__batch_size__*(0.5-self.ratio)/self.num_class)\n",
    "            \n",
    "            Xs = torch.LongTensor([])\n",
    "            Ys = torch.LongTensor([])\n",
    "                        \n",
    "            for i in range(self.num_class):\n",
    "                \n",
    "                if len(self.clusters[i])==0:\n",
    "                    continue\n",
    "                \n",
    "                indices = torch.randint(len(self.clusters[i]), (per_class, ))\n",
    "                x = self.clusters[i][indices]\n",
    "                indices = torch.randint(len(self.clusters[i]), (per_class, ))\n",
    "                y = self.clusters[i][indices]\n",
    "                Xs = torch.cat((Xs,x))\n",
    "                Ys = torch.cat((Ys,y))                  \n",
    "            \n",
    "            remaining = per_class*self.num_class\n",
    "            remaining = self.__batch_size__ - remaining\n",
    "            \n",
    "            indices = torch.randint(len(self.input_nodeidx), (remaining, ))\n",
    "            x = self.input_nodeidx[indices]\n",
    "            indices = torch.randint(len(self.input_nodeidx), (remaining, ))\n",
    "            y = self.input_nodeidx[indices]\n",
    "            \n",
    "            x = torch.cat((Xs,x))\n",
    "            y = torch.cat((Ys,y))\n",
    "                    \n",
    "            #print(x.shape)\n",
    "            #print(y.shape)\n",
    "            \n",
    "        else:\n",
    "            indices = torch.randint(len(self.input_nodeidx), (self.__batch_size__, ))\n",
    "            x = self.input_nodeidx[indices]\n",
    "            indices = torch.randint(len(self.input_nodeidx), (self.__batch_size__, ))\n",
    "            y = self.input_nodeidx[indices]\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __collate__(self, data_list):\n",
    "        assert len(data_list) == 1\n",
    "        \n",
    "        x, y = data_list[0]\n",
    "        \n",
    "        label = (self.data.y[x] == self.data.y[y]).type(torch.float)\n",
    "        b_data = self.data.__class__()\n",
    "        b_data.x = x\n",
    "        b_data.y = y\n",
    "        b_data.label = label\n",
    "        b_data.x_feat = self.data.x[x]\n",
    "        b_data.y_feat = self.data.x[y]\n",
    "                \n",
    "        return b_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef05b10",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18ca9024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data, dataset = get_data('Reddit', log=False)\n",
    "\n",
    "# from torch_geometric.data import Data\n",
    "# x = torch.Tensor([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1],[0,1]])\n",
    "# y = torch.LongTensor([0,0,0, 1, 1, 1, 1])\n",
    "# edge_index = torch.LongTensor([[1,2],[1,4],[1,5],[2,1],[3,6],[3,7],[4,5],[4,1],[4,6],[4,7],[5,1],[5,4],[5,6],[6,3],[6,4],[6,5],[6,7],[7,3],[7,4],[7,6]]).T\n",
    "# edge_index = edge_index-1\n",
    "# train_mask = torch.zeros(len(y)).type(torch.bool)\n",
    "# train_mask[[0,1,2]]=True\n",
    "# data = Data(x=x, y=y, edge_index = edge_index, train_mask = train_mask, val_mask = train_mask, test_mask = train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4649fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 4096\n",
    "# train_sampler  = LinkSampler(data, input_nodes = data.train_mask, batch_size = batch_size, num_steps = 10, save_dir = None,\n",
    "#                              recompute = True,log = True, balance=True)\n",
    "# for batch in train_sampler:\n",
    "#     print(batch)\n",
    "#     print(sum(batch.label)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "089a539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LinkModel(data.num_features, num_neurons=64).to(device)\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026939c",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e809c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, sampler, log = True):\n",
    "\n",
    "    y_pred=np.array([])\n",
    "    y_true=np.array([])\n",
    "    \n",
    "    if log:\n",
    "        pbar = tqdm(total=args.link_batch_size*args.link_num_steps)\n",
    "        pbar.set_description(f'Predicting: ')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for b_data in sampler:\n",
    "            b_data = b_data.to(device)\n",
    "            out = model(b_data.x_feat, b_data.y_feat).view(-1)         \n",
    "            \n",
    "            pred = torch.zeros_like(out)\n",
    "            pred[out >= 0.5] = 1\n",
    "            \n",
    "            pred = pred.cpu().numpy()\n",
    "            test_target=b_data.label.cpu().numpy()\n",
    "        \n",
    "#             print(out)\n",
    "#             print(pred)\n",
    "#             print(test_target)\n",
    "        \n",
    "            y_pred = np.append(y_pred,pred)\n",
    "            y_true = np.append(y_true,test_target)\n",
    "            \n",
    "            if log:\n",
    "                pbar.update(args.link_batch_size)\n",
    "        if log:\n",
    "            pbar.close()\n",
    "    \n",
    "    micro=f1_score(y_true, y_pred, average='micro')\n",
    "    weighted=f1_score(y_true, y_pred, average='weighted')\n",
    "    acc=accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    return acc, micro, weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08c7daed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, data, log = True, epochs=1, worker = 0):    \n",
    "        \n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = nn.MSELoss()    \n",
    "#     criterion = nn.BCELoss()\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     criterion = nn.NLLLoss()\n",
    "            \n",
    "    train_sampler  = LinkSampler(data, input_nodes = data.train_mask, batch_size = args.link_batch_size, num_steps = args.link_num_steps,\n",
    "                                 save_dir = None, recompute = False,log = log, balance=args.balance, num_workers = worker)\n",
    "    \n",
    "    val_sampler  = LinkSampler(data, input_nodes = data.val_mask, batch_size = args.link_batch_size, num_steps = args.link_num_steps,\n",
    "                                 save_dir = None, recompute = False,log = log, balance=args.balance, num_workers = worker)\n",
    "    \n",
    "    test_sampler  = LinkSampler(data, input_nodes = data.test_mask, batch_size = args.link_batch_size, num_steps = args.link_num_steps,\n",
    "                                 save_dir = None, recompute = False,log = log, balance=args.balance, num_workers = worker)\n",
    "    \n",
    "    show_pbar = log\n",
    "#     if log and data.num_nodes>100000:\n",
    "#         show_pbar = True\n",
    "    \n",
    "    #worker = 0     \n",
    "    train_losses=[]\n",
    "    \n",
    "    for epoch in range(1,epochs+1):        \n",
    "        total_loss = total_examples = 0\n",
    "        y_pred=[]\n",
    "        y_true=[]\n",
    "        \n",
    "        if show_pbar:\n",
    "            pbar = tqdm(total=args.link_batch_size*args.link_num_steps)\n",
    "            pbar.set_description(f'Epoch {epoch:02d}')\n",
    "        \n",
    "        model.train()\n",
    "        for b_data in train_sampler:            \n",
    "            b_data = b_data.to(device)\n",
    "            \n",
    "#             print(sum(b_data.label))\n",
    "            \n",
    "            optimizer.zero_grad()            \n",
    "            out = model(b_data.x_feat,b_data.y_feat)\n",
    "            loss = criterion(out, b_data.label.view(-1,1))            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_examples += args.link_batch_size\n",
    "            \n",
    "            if show_pbar:\n",
    "                pbar.update(args.link_batch_size)\n",
    "        if show_pbar:\n",
    "            pbar.close()\n",
    "        \n",
    "        loss=total_loss / args.link_num_steps\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        if log:            \n",
    "            print(f'Epoch {epoch:03d} Loss {loss:.4f}', end=' ')            \n",
    "            tr_a, tr_b, tr_c = predict(model, data, train_sampler, log = log)\n",
    "            print(f'\\t{tr_a:.4f},\\t{tr_b:.4f},\\t{tr_c:.4f}')            \n",
    "    \n",
    "    if log:\n",
    "#         tr_a, tr_b, tr_c = predict(model, data, train_sampler, log = log) \n",
    "#         print(f'Train\\t{tr_a:.4f},\\t{tr_b:.4f},\\t{tr_c:.4f}')\n",
    "        tr_a, tr_b, tr_c = predict(model, data, val_sampler, log = log) \n",
    "        print(f'Val\\t{tr_a:.4f},\\t{tr_b:.4f},\\t{tr_c:.4f}')\n",
    "        tr_a, tr_b, tr_c = predict(model, data, test_sampler, log = log) \n",
    "        print(f'Test\\t{tr_a:.4f},\\t{tr_b:.4f},\\t{tr_c:.4f}')\n",
    "                \n",
    "    return model\n",
    "\n",
    "# train(model, data, log = True, epochs=100)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23a4957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_link(data, selfloop = True, log = True, worker = 0):\n",
    "    \n",
    "    args.balance = False\n",
    "    \n",
    "    if data.num_nodes<10000:\n",
    "        args.epochs = 20\n",
    "        args.num_neurons = 32\n",
    "        args.link_batch_size = min(4096, data.num_nodes*data.num_nodes)\n",
    "        args.link_num_steps = 200\n",
    "\n",
    "    elif data.num_nodes<100000:\n",
    "        args.epochs = 10\n",
    "        args.num_neurons = 32\n",
    "        args.link_batch_size = 4096*2\n",
    "        args.link_num_steps = 200\n",
    "    else:\n",
    "        args.epochs = 5\n",
    "        args.num_neurons = 32\n",
    "        args.link_batch_size = 4096*8\n",
    "        args.link_num_steps = 200\n",
    "        \n",
    "    #args.epochs = 1\n",
    "    \n",
    "        \n",
    "    model = LinkModel(data.num_features, num_neurons=args.num_neurons).to(device)\n",
    "    \n",
    "    if log:\n",
    "        print(model)\n",
    "    \n",
    "    train(model, data, log = log, epochs=args.epochs, worker = worker)        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1caad",
   "metadata": {},
   "source": [
    "## Edge Weight Computation Edge Wise (Fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91971043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_weight(data, selfloop = True, log = True, worker = 0):\n",
    "    \n",
    "    #if data.num_nodes<10000:\n",
    "    worker = 0 ##worker 0 found to be fastest\n",
    "    \n",
    "    link_model = train_link(data, selfloop = selfloop, log = log, worker = worker)\n",
    "    \n",
    "    w = torch.Tensor([]).type(torch.float).to(device)\n",
    "    \n",
    "    indices = torch.arange(0, data.edge_index.shape[1])\n",
    "    batches = torch.split(indices, args.link_batch_size)\n",
    "    \n",
    "    link_model.eval()    \n",
    "    with torch.no_grad():    \n",
    "        for batch in batches:\n",
    "            \n",
    "            #print(batch)\n",
    "            \n",
    "            idx = data.edge_index[:,batch]\n",
    "            ew = link_model(data.x[idx[0]].to(device),data.x[idx[1]].to(device))\n",
    "            ew = ew.view(-1)\n",
    "            ew = torch.clamp(ew, min=1e-3, max=1.0)\n",
    "            w = torch.cat((w,ew))\n",
    "            \n",
    "    return w.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab51012",
   "metadata": {},
   "source": [
    "## Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95a70dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, dataset = get_data('Reddit', log=False, h_score=True, split_no = 0)\n",
    "# args.epochs = 1\n",
    "# args.num_neurons = 32\n",
    "# args.link_batch_size = 4096*8\n",
    "# args.link_num_steps = 100\n",
    "# args.balance = True\n",
    "\n",
    "# start = time.time()\n",
    "# link_model = train_link(data, selfloop = True, log = True)\n",
    "# end = time.time()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfefd237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = get_link_weight(data, selfloop = True, log = False, worker=0)\n",
    "# print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb89da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4230836",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Weight assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d20ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkNN():\n",
    "    \n",
    "    def __init__(self, data, value='min', log=True, worker=0, \n",
    "                 lambda1=0.25, lambda2=0.25, w1=1.0, w2=0.5, w3=0.1):\n",
    "        \n",
    "        self.N = N = data.num_nodes\n",
    "        self.E = E = data.num_edges\n",
    "        self.data = data        \n",
    "        self.value = value\n",
    "        self.log = log\n",
    "        self.lambda1=lambda1\n",
    "        self.lambda2=lambda2\n",
    "        self.w1=w1\n",
    "        self.w2=w2\n",
    "        self.w3=w3\n",
    "        \n",
    "        self.sign = 1\n",
    "        \n",
    "        if value=='min':\n",
    "            self.sign = -1\n",
    "            \n",
    "        self.adj = SparseTensor(\n",
    "            row=data.edge_index[0], col=data.edge_index[1],\n",
    "            value=torch.arange(E, device=data.edge_index.device),\n",
    "            sparse_sizes=(N, N))\n",
    "        \n",
    "        self.weight = get_link_weight(data, selfloop = True, log = log, worker=worker)\n",
    "        \n",
    "    def node_weight(self,u):\n",
    "    \n",
    "        row, col, edge_index = self.adj[u,:].coo()           \n",
    "        \n",
    "        target_class_sim = self.weight[edge_index]\n",
    "        ind = np.argsort(self.sign*target_class_sim) #-1*desending, normal will be ascending\n",
    "        \n",
    "#         print(u, row, col, edge_index)\n",
    "#         print(target_class_sim)\n",
    "#         print(ind)\n",
    "         \n",
    "        lambda1 = self.lambda1 #top 25% with probability 1\n",
    "        lambda2 = self.lambda2 #second 25% with probability 0.5 \n",
    "        \n",
    "        l1=math.ceil(len(col)*lambda1)\n",
    "        l2=min(len(col)-l1,math.ceil(len(col)*lambda2))        \n",
    "        l3=max(0,int(len(col)-l1-l2))\n",
    "        \n",
    "#         print(len(col),l1, l2, l3)\n",
    "        \n",
    "#         S_G = np.ones(l1, dtype=float)*1.0\n",
    "#         S_G = np.append(S_G, np.ones(l2, dtype=float)*0.5)\n",
    "#         if(l3>0):\n",
    "#             S_G = np.append(S_G, np.ones(l3, dtype=float)*0.1)\n",
    "\n",
    "        S_G = np.ones(l1, dtype=float)*self.w1\n",
    "        S_G = np.append(S_G, np.ones(l2, dtype=float)*self.w2)\n",
    "        \n",
    "        if(l3>0):\n",
    "            S_G = np.append(S_G, np.ones(l3, dtype=float)*self.w3)\n",
    "        \n",
    "        S_G = S_G.tolist()\n",
    "        \n",
    "#         S_G = list(range(1,len(col)+1))\n",
    "        S_edge = edge_index[ind].tolist()\n",
    "        \n",
    "        return S_G, S_edge\n",
    "\n",
    "    def get_nn_weight(self):\n",
    "        \n",
    "        if self.log:        \n",
    "            pbar = tqdm(total=self.N)\n",
    "            pbar.set_description(f'Nodes')\n",
    "\n",
    "        edge_weight=[]\n",
    "        edge_index=[]\n",
    "\n",
    "        for u in range(self.N):            \n",
    "            weight, e_index = self.node_weight(u)\n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            if self.log:        \n",
    "                pbar.update(1)\n",
    "        if self.log:        \n",
    "            pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E\n",
    "        \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)\n",
    "        \n",
    "        return weight\n",
    "    \n",
    "    def process_block(self, list_u):\n",
    "        \n",
    "        #print(\"Processing :\",len(list_u), list_u[0], list_u[-1])\n",
    "        \n",
    "        edge_weight = []\n",
    "        edge_index = []\n",
    "        \n",
    "        for u in list_u:        \n",
    "            weight, e_index = self.node_weight(u)            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            \n",
    "        #print(\"Done :\",len(list_u), list_u[0], list_u[-1])\n",
    "            \n",
    "        return edge_weight, edge_index, len(list_u)\n",
    "    \n",
    "    #multiprocessing\n",
    "    def get_nn_weight_multiproces(self):\n",
    "        \n",
    "        edge_weight=[]\n",
    "        edge_index=[]        \n",
    "        \n",
    "        N = self.N\n",
    "        num_blocks = NUM_PROCESSORS\n",
    "        elem_size = int(N/num_blocks)\n",
    "\n",
    "        \n",
    "        nodes = np.arange(num_blocks*elem_size).reshape(num_blocks,-1).tolist()\n",
    "        if num_blocks*elem_size<N:\n",
    "            nodes.append(list(range(num_blocks*elem_size,N)))        \n",
    "        \n",
    "        pool_size = NUM_PROCESSORS        \n",
    "        if self.log:\n",
    "            print(\"Pool Size: \", pool_size)        \n",
    "        pool = Pool(pool_size)\n",
    "        \n",
    "        if self.log:\n",
    "            pbar = tqdm(total=N)\n",
    "            pbar.set_description(f'Nodes')  \n",
    "                \n",
    "        for (weight, e_index, num_el) in pool.imap_unordered(self.process_block, nodes):            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            if self.log:\n",
    "                pbar.update(num_el)\n",
    "        if self.log:\n",
    "            pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E        \n",
    "        \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)\n",
    "        \n",
    "        return weight\n",
    "    \n",
    "    \n",
    "    def compute_weights(self):\n",
    "        if self.data.num_nodes<10000:\n",
    "            weight = self.get_nn_weight()    \n",
    "        else:\n",
    "            weight = self.get_nn_weight_multiproces()\n",
    "        \n",
    "        return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c7e7c",
   "metadata": {},
   "source": [
    "## Submodular Weight assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bbe80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkSub():\n",
    "    \n",
    "    def __init__(self, data, value='max', selfloop = False, log = True, worker=0, lambda1=0.25, lambda2=0.25, w1=1.0, w2=0.5, w3=0.1):\n",
    "        \n",
    "        self.N = N = data.num_nodes\n",
    "        self.E = E = data.num_edges\n",
    "        self.data = data\n",
    "        self.log = log\n",
    "        self.selfloop = selfloop\n",
    "        \n",
    "        self.lambda1=lambda1\n",
    "        self.lambda2=lambda2\n",
    "        self.w1=w1\n",
    "        self.w2=w2\n",
    "        self.w3=w3\n",
    "    \n",
    "        #self.X = data.x.to(device)\n",
    "        self.X = self.data.x\n",
    "        self.on_device=True        \n",
    "        \n",
    "        self.model = train_link(data, selfloop = selfloop, log= log, worker=worker)\n",
    "        self.model.eval()        \n",
    "\n",
    "        self.adj = SparseTensor(\n",
    "            row=data.edge_index[0], col=data.edge_index[1],\n",
    "            value=torch.arange(E, device=data.edge_index.device),\n",
    "            sparse_sizes=(N, N))\n",
    "        \n",
    "        if self.log:\n",
    "            print(\"value: \", value)\n",
    "        \n",
    "        self.value = value\n",
    "        self.sign = -1\n",
    "        \n",
    "        if self.value == 'max':\n",
    "            self.sign = 1 ##-1 select the nearest ones, 1 for the farthest        \n",
    "            \n",
    "        elif self.value == 'min':\n",
    "            self.sign = -1\n",
    "        else:\n",
    "            raise 'Not implemented error'\n",
    "    \n",
    "    def pairwise_link(self, x):  \n",
    "                \n",
    "        n, f = x.shape\n",
    "        \n",
    "        x_col1 = x.repeat_interleave(n, dim=0)\n",
    "        x_col2 = x.repeat(n,1)\n",
    "        # print(x_col1, x_col2)\n",
    "        \n",
    "        if self.on_device:\n",
    "            x_col1 = x_col1.to(device)\n",
    "            x_col2 = x_col2.to(device)            \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.model(x_col1, x_col2).detach().cpu()\n",
    "            output = torch.clamp(output, min=1e-3, max=1.0)\n",
    "        #print(output.shape)\n",
    "#         output = output.softmax(dim=1)\n",
    "#         second_column = output[:,1].cpu()        \n",
    "        #print(second_column)\n",
    "        \n",
    "        similarity_matrix = output.view(n,n)\n",
    "        \n",
    "#         print(similarity_matrix)\n",
    "        \n",
    "        return similarity_matrix\n",
    "        \n",
    "    def lazy_greedy_weight(self,u):\n",
    "        \n",
    "        row, col, edge_index = self.adj[u,:].coo()\n",
    "        \n",
    "        if len(col)==0:\n",
    "            return [],[]\n",
    "        \n",
    "        \n",
    "        vertices = [u]+col.tolist()\n",
    "        \n",
    "        v2i={i:j for i,j in zip(vertices, range(len(vertices)))}\n",
    "        i2v={value:key for key, value in v2i.items()}\n",
    "        \n",
    "        kernel_dist = self.pairwise_link(self.X[vertices])\n",
    "        \n",
    "        gain_list=[(self.sign*kernel_dist[v2i[u],v2i[v.item()]],v.item(), e.item()) for v,e in zip(col,edge_index)] \n",
    "        #-1 selecting nearest\n",
    "        #1 selecting farthest\n",
    "\n",
    "        heapq.heapify(gain_list)\n",
    "        #print(gain_list)\n",
    "\n",
    "        S=[u]\n",
    "        S_G=[]\n",
    "        S_edge=[]\n",
    "        S_index=[v2i[u]]\n",
    "        \n",
    "        lambda1 = self.lambda1 #top 25% with probability 1\n",
    "        lambda2 = self.lambda2 #second 25% with probability 0.5         \n",
    "        l1=math.ceil(len(col)*lambda1)\n",
    "        l2=min(len(col)-l1,math.ceil(len(col)*lambda2))\n",
    "        l3=max(0,int(len(col)-l1-l2))\n",
    "        \n",
    "        #print(len(col),l1, l2, l3)\n",
    "        \n",
    "        rank=1 #rank weight instead gain weight\n",
    "        \n",
    "        while(gain_list):\n",
    "            (gain_v, v, e) = heapq.heappop(gain_list)\n",
    "            gain_v = self.sign*gain_v #make it positive\n",
    "            #print(gain_v, v)\n",
    "\n",
    "            if len(gain_list)==0:                                    \n",
    "                S.append(v)\n",
    "                #if gain_v<1e-6:gain_v=1e-6#S_G.append(gain_v)#S_G.append(rank)\n",
    "                if rank <= l1:S_G.append(self.w1)                \n",
    "                elif rank<=l1+l2:S_G.append(self.w2)\n",
    "                else:S_G.append(self.w3)\n",
    "\n",
    "                rank+=1                \n",
    "                S_edge.append(e)\n",
    "                S_index.append(v2i[v])\n",
    "                break\n",
    "            elif len(gain_list)<l3:\n",
    "                S.append(v)\n",
    "                S_G.append(self.w3)\n",
    "                rank+=1                \n",
    "                S_edge.append(e)\n",
    "                S_index.append(v2i[v])\n",
    "                continue\n",
    "            \n",
    "            gain_v_update = self.sign*min(kernel_dist[v2i[v],S_index])\n",
    "            \n",
    "            #print(\"updated: \", S,v,gain_v_update, gain_v)\n",
    "            (gain_v_second,v_second,_)=gain_list[0] #top\n",
    "            gain_v_second = gain_v_second #make it positive\n",
    "\n",
    "            if gain_v_update<=gain_v_second:\n",
    "                \n",
    "                gain_v_update = self.sign*gain_v_update\n",
    "                \n",
    "                if gain_v_update<1e-6:\n",
    "                    gain_v_update=1e-6\n",
    "                S.append(v)\n",
    "                #S_G.append(gain_v_update)\n",
    "                #S_G.append(rank)\n",
    "                \n",
    "                if rank<=l1:\n",
    "                    S_G.append(self.w1)\n",
    "                elif rank<=l1+l2:\n",
    "                    S_G.append(self.w2)\n",
    "                else:\n",
    "                    S_G.append(self.w3)\n",
    "                rank+=1\n",
    "                \n",
    "                S_edge.append(e)\n",
    "                S_index.append(v2i[v])\n",
    "            else:\n",
    "                heapq.heappush(gain_list,(self.sign*gain_v_update,v, e))\n",
    "\n",
    "        return S_G, S_edge\n",
    "    \n",
    "    #serial\n",
    "    def get_submodular_weight(self):\n",
    "        \n",
    "        N = self.N\n",
    "        #N = 1000\n",
    "        \n",
    "        if self.log:\n",
    "            pbar = tqdm(total=N)\n",
    "            pbar.set_description(f'Nodes')\n",
    "\n",
    "        edge_weight=[]\n",
    "        edge_index=[]\n",
    "        \n",
    "        test = 0\n",
    "\n",
    "        for u in range(N):                \n",
    "            weight, e_index = self.lazy_greedy_weight(u)\n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "        \n",
    "            #test += sum((np.array(weight)>1.0).astype(int))\n",
    "            if self.log:\n",
    "                pbar.update(1)\n",
    "        \n",
    "        #print(test)\n",
    "        if self.log:\n",
    "            pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E        \n",
    "        \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)\n",
    "        \n",
    "        return weight\n",
    "        \n",
    "    \n",
    "    def process_block(self, list_u):\n",
    "        \n",
    "        #print(\"Processing :\",len(list_u), list_u[0], list_u[-1])\n",
    "        \n",
    "        edge_weight = []\n",
    "        edge_index = []\n",
    "        \n",
    "        for u in list_u:        \n",
    "            weight, e_index = self.lazy_greedy_weight(u)            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            \n",
    "        #print(\"Done :\",len(list_u), list_u[0], list_u[-1])\n",
    "            \n",
    "        return edge_weight, edge_index, len(list_u)\n",
    "        \n",
    "    \n",
    "    #multiprocessing\n",
    "    def get_submodular_weight_multiproces(self):\n",
    "        \n",
    "        edge_weight=[]\n",
    "        edge_index=[]        \n",
    "        \n",
    "        N = self.N\n",
    "        num_blocks = NUM_PROCESSORS\n",
    "        elem_size = int(N/num_blocks)\n",
    "        \n",
    "        nodes = np.arange(num_blocks*elem_size).reshape(num_blocks,-1).tolist()\n",
    "        if num_blocks*elem_size<N:\n",
    "            nodes.append(list(range(num_blocks*elem_size,N)))        \n",
    "        \n",
    "        pool_size = NUM_PROCESSORS        \n",
    "        if self.log:\n",
    "            print(\"Pool Size: \", pool_size)        \n",
    "        pool = Pool(pool_size)\n",
    "        \n",
    "        if self.log:\n",
    "            pbar = tqdm(total=N)\n",
    "            pbar.set_description(f'Nodes')  \n",
    "                \n",
    "        for (weight, e_index, num_el) in pool.imap_unordered(self.process_block, nodes):            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            \n",
    "            if self.log:\n",
    "                pbar.update(num_el)\n",
    "        \n",
    "        if self.log:\n",
    "            pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E\n",
    "                \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)        \n",
    "        \n",
    "        return weight\n",
    "    \n",
    "    \n",
    "    def compute_weights(self):  \n",
    "        \n",
    "        if self.data.num_nodes<10000:\n",
    "            weight = self.get_submodular_weight()\n",
    "        else:      \n",
    "            self.on_device=False\n",
    "            self.X = self.data.x.to('cpu')        \n",
    "            self.model = self.model.to('cpu')            \n",
    "            self.model.eval()\n",
    "            #weight = self.get_submodular_weight_multiproces()\n",
    "            weight = self.get_submodular_weight()    \n",
    "        \n",
    "        return weight\n",
    "    \n",
    "# data, dataset = get_data('karate', log=False)\n",
    "# submodular_weight = LinkSub(data, selfloop = False, log = True)\n",
    "# submodular_weight.lazy_greedy_weight(1)\n",
    "# #data.weight = submodular_weight.compute_weights()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c8e1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, dataset = get_data('karate', log = False)\n",
    "# submodular_weight = LinkSub(data, log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d24d2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submodular_weight.lazy_greedy_weight(0)\n",
    "#data.weight = submodular_weight.compute_weights()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddbb922",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec49c506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:  /scratch/gilbreth/das90/Dataset/\n",
      "Result directory: /scratch/gilbreth/das90/Dataset/RESULTS/\n",
      "\n",
      "Dataset: Genius(1):\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 12\n",
      "Number of classes: 2\n",
      "\n",
      "Data(x=[421961, 12], edge_index=[2, 984979], y=[421961], train_mask=[421961], val_mask=[421961], test_mask=[421961])\n",
      "===========================================================================================================\n",
      "Number of nodes: 421961\n",
      "Number of edges: 984979\n",
      "Average node degree: 2.33\n",
      "Number of training nodes: 253176\n",
      "Training node label rate: 0.60\n",
      "Has isolated nodes: True\n",
      "Has self-loops: False\n",
      "Is undirected: False\n",
      "N  421961  E  984979  d  2.3342891878633334 0.47737330198287964 0.617572546005249 0.21700388193130493 -0.1065090000629425 LinkModel(\n",
      "  (MLP1): Linear(in_features=12, out_features=32, bias=True)\n",
      "  (MLP3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01: 100%|██████████| 6553600/6553600 [00:01<00:00, 3797528.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 Loss 216328286648.3200 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 6553600/6553600 [00:02<00:00, 2696771.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0.6800,\t0.6800,\t0.6837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02: 100%|██████████| 6553600/6553600 [00:01<00:00, 3838315.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 Loss 191739038387.6000 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 6553600/6553600 [00:02<00:00, 2678031.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0.6908,\t0.6908,\t0.7012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 03: 100%|██████████| 6553600/6553600 [00:01<00:00, 3850666.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 Loss 37825787857.8400 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 6553600/6553600 [00:02<00:00, 2669601.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0.6914,\t0.6914,\t0.7009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 04: 100%|██████████| 6553600/6553600 [00:01<00:00, 4516733.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 Loss 84217368001.6800 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 6553600/6553600 [00:02<00:00, 2750183.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0.6735,\t0.6735,\t0.6779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 05: 100%|██████████| 6553600/6553600 [00:01<00:00, 4500858.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 Loss 575879590148.1200 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 6553600/6553600 [00:02<00:00, 2938776.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0.6803,\t0.6803,\t0.6838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 6553600/6553600 [00:02<00:00, 3206506.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val\t0.6794,\t0.6794,\t0.6829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 6553600/6553600 [00:02<00:00, 3223053.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\t0.6798,\t0.6798,\t0.6835\n",
      "Execution time:  71.15590238571167\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "    \n",
    "    log = True\n",
    "    datasetname = 'genius'\n",
    "    #args.link_batch_size = 32\n",
    "    args.epochs = 2\n",
    "    \n",
    "    data, dataset = get_data(datasetname, log=log, h_score=True)\n",
    "    #data = generate_synthetic(data, d=100, h=0.25, train=0.1, random_state=1, log=log)\n",
    "    #--------------------------#\n",
    "    start = time.time() \n",
    "    data.weight = get_link_weight(data, selfloop = True, log = log, worker = 0)\n",
    "    end = time.time()\n",
    "    print(\"Execution time: \", end-start)\n",
    "#     #--------------------------#\n",
    "#     start = time.time()     \n",
    "#     nn_weight = LinkNN(data, value ='min', log = log) \n",
    "#     data.weight = nn_weight.compute_weights()    \n",
    "#     end = time.time()\n",
    "#     print(\"Execution time: \", end-start)\n",
    "#     #--------------------------#\n",
    "#     start = time.time()    \n",
    "#     submodular_weight = LinkSub(data, value ='max', selfloop = True, log = log)    \n",
    "#     data.weight = submodular_weight.compute_weights()    \n",
    "#     end = time.time()\n",
    "#     print(\"Execution time: \", end-start)\n",
    "    #--------------------------#    \n",
    "#     if 'weight' in data:\n",
    "#         cp_data= copy.deepcopy(data)\n",
    "#         G = to_networkx(cp_data, to_undirected=True, edge_attrs=['weight'])\n",
    "#         to_remove = [(a,b) for a, b, attrs in G.edges(data=True) if attrs[\"weight\"] <1.0 ]\n",
    "#         G.remove_edges_from(to_remove)\n",
    "#         updated_data = from_networkx(G)\n",
    "        \n",
    "#         print(updated_data)\n",
    "        \n",
    "#         updated_data = from_networkx(G, group_edge_attrs=['weight'])\n",
    "#         updated_data.weight = updated_data.edge_attr.view(-1)\n",
    "        \n",
    "#         row, col = updated_data.edge_index\n",
    "#         updated_data.edge_index = torch.stack((torch.cat((row, col),dim=0), torch.cat((col, row),dim=0)),dim=0)\n",
    "#         updated_data.weight = torch.cat((updated_data.weight, updated_data.weight),dim=0)\n",
    "        \n",
    "#         print(\"Node Homophily:\", homophily(updated_data.edge_index, cp_data.y, method='node'))\n",
    "#         print(\"Edge Homophily:\", homophily(updated_data.edge_index, cp_data.y, method='edge'))\n",
    "#         print(\"Edge_insensitive Homophily:\", homophily(updated_data.edge_index, cp_data.y, method='edge_insensitive'))    \n",
    "        \n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3092d167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
